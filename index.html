<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2510.06669.pdf' target='_blank'>https://arxiv.org/pdf/2510.06669.pdf</a></span>   <span><a href='https://github.com/Yuxi104/AutoNAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxi Liu, Yunfeng Ma, Yi Tang, Min Liu, Shuai Jiang, Yaonan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06669">Automated Neural Architecture Design for Industrial Defect Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial surface defect detection (SDD) is critical for ensuring product quality and manufacturing reliability. Due to the diverse shapes and sizes of surface defects, SDD faces two main challenges: intraclass difference and interclass similarity. Existing methods primarily utilize manually designed models, which require extensive trial and error and often struggle to address both challenges effectively. To overcome this, we propose AutoNAD, an automated neural architecture design framework for SDD that jointly searches over convolutions, transformers, and multi-layer perceptrons. This hybrid design enables the model to capture both fine-grained local variations and long-range semantic context, addressing the two key challenges while reducing the cost of manual network design. To support efficient training of such a diverse search space, AutoNAD introduces a cross weight sharing strategy, which accelerates supernet convergence and improves subnet performance. Additionally, a searchable multi-level feature aggregation module (MFAM) is integrated to enhance multi-scale feature learning. Beyond detection accuracy, runtime efficiency is essential for industrial deployment. To this end, AutoNAD incorporates a latency-aware prior to guide the selection of efficient architectures. The effectiveness of AutoNAD is validated on three industrial defect datasets and further applied within a defect imaging and detection platform. Code will be available at https://github.com/Yuxi104/AutoNAD.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2509.17670.pdf' target='_blank'>https://arxiv.org/pdf/2509.17670.pdf</a></span>   <span><a href='https://github.com/marietteschonfeld/LWinNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mariette Schönfeld, Wannes Meert, Hendrik Blockeel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17670">Tailored Transformation Invariance for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial Anomaly Detection (IAD) is a subproblem within Computer Vision Anomaly Detection that has been receiving increasing amounts of attention due to its applicability to real-life scenarios. Recent research has focused on how to extract the most informative features, contrasting older kNN-based methods that use only pretrained features. These recent methods are much more expensive to train however and could complicate real-life application. Careful study of related work with regards to transformation invariance leads to the idea that popular benchmarks require robustness to only minor translations. With this idea we then formulate LWinNN, a local window based approach that creates a middle ground between kNN based methods that have either complete or no translation invariance. Our experiments demonstrate that this small change increases accuracy considerably, while simultaneously decreasing both train and test time. This teaches us two things: first, the gap between kNN-based approaches and more complex state-of-the-art methodology can still be narrowed by effective usage of the limited data available. Second, our assumption of requiring only limited translation invariance highlights potential areas of interest for future work and the need for more spatially diverse benchmarks, for which our method can hopefully serve as a new baseline. Our code can be found at https://github.com/marietteschonfeld/LWinNN .
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2508.09178.pdf' target='_blank'>https://arxiv.org/pdf/2508.09178.pdf</a></span>   <span><a href='https://github.com/Yanhui-Lee/IAD-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanhui Li, Yunkang Cao, Chengliang Liu, Yuan Xiong, Xinghui Dong, Chao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09178">IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection is a critical component of modern manufacturing, yet the scarcity of defective samples restricts traditional detection methods to scenario-specific applications. Although Vision-Language Models (VLMs) demonstrate significant advantages in generalization capabilities, their performance in industrial anomaly detection remains limited. To address this challenge, we propose IAD-R1, a universal post-training framework applicable to VLMs of different architectures and parameter scales, which substantially enhances their anomaly detection capabilities. IAD-R1 employs a two-stage training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT) stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset (Expert-AD) for training, enhancing anomaly perception capabilities and establishing reasoning-to-answer correlations; the Structured Control Group Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward functions to achieve a capability leap from "Anomaly Perception" to "Anomaly Interpretation". Experimental results demonstrate that IAD-R1 achieves significant improvements across 7 VLMs, the largest improvement was on the DAGM dataset, with average accuracy 43.3% higher than the 0.5B baseline. Notably, the 0.5B parameter model trained with IAD-R1 surpasses commercial models including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the effectiveness and superiority of IAD-R1. The dataset, code, and all model weights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2507.19253.pdf' target='_blank'>https://arxiv.org/pdf/2507.19253.pdf</a></span>   <span><a href='https://github.com/Xantastic/BridgeNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>An Xiang, Zixuan Huang, Xitong Gao, Kejiang Ye, Cheng-zhong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19253">BridgeNet: A Unified Multimodal Framework for Bridging 2D and 3D Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection for 2D objects has gained significant attention and achieved progress in anomaly detection (AD) methods. However, identifying 3D depth anomalies using only 2D information is insufficient. Despite explicitly fusing depth information into RGB images or using point cloud backbone networks to extract depth features, both approaches struggle to adequately represent 3D information in multimodal scenarios due to the disparities among different modal information. Additionally, due to the scarcity of abnormal samples in industrial data, especially in multimodal scenarios, it is necessary to perform anomaly generation to simulate real-world abnormal samples. Therefore, we propose a novel unified multimodal anomaly detection framework to address these issues. Our contributions consist of 3 key aspects. (1) We extract visible depth information from 3D point cloud data simply and use 2D RGB images to represent appearance, which disentangles depth and appearance to support unified anomaly generation. (2) Benefiting from the flexible input representation, the proposed Multi-Scale Gaussian Anomaly Generator and Unified Texture Anomaly Generator can generate richer anomalies in RGB and depth. (3) All modules share parameters for both RGB and depth data, effectively bridging 2D and 3D anomaly detection. Subsequent modules can directly leverage features from both modalities without complex fusion. Experiments show our method outperforms state-of-the-art (SOTA) on MVTec-3D AD and Eyecandies datasets. Code available at: https://github.com/Xantastic/BridgeNet
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2507.07939.pdf' target='_blank'>https://arxiv.org/pdf/2507.07939.pdf</a></span>   <span><a href='https://github.com/amoreZgx1n/SAGE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoxin Zang, Xue Li, Donglin Di, Lanshun Nie, Dechen Zhan, Yang Song, Lei Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07939">SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While Vision-Language Models (VLMs) have shown promising progress in general multimodal tasks, they often struggle in industrial anomaly detection and reasoning, particularly in delivering interpretable explanations and generalizing to unseen categories. This limitation stems from the inherently domain-specific nature of anomaly detection, which hinders the applicability of existing VLMs in industrial scenarios that require precise, structured, and context-aware analysis. To address these challenges, we propose SAGE, a VLM-based framework that enhances anomaly reasoning through Self-Guided Fact Enhancement (SFE) and Entropy-aware Direct Preference Optimization (E-DPO). SFE integrates domain-specific knowledge into visual reasoning via fact extraction and fusion, while E-DPO aligns model outputs with expert preferences using entropy-aware optimization. Additionally, we introduce AD-PL, a preference-optimized dataset tailored for industrial anomaly reasoning, consisting of 28,415 question-answering instances with expert-ranked responses. To evaluate anomaly reasoning models, we develop Multiscale Logical Evaluation (MLE), a quantitative framework analyzing model logic and consistency. SAGE demonstrates superior performance on industrial anomaly datasets under zero-shot and one-shot settings. The code, model and dataset are available at https://github.com/amoreZgx1n/SAGE.
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2506.05721.pdf' target='_blank'>https://arxiv.org/pdf/2506.05721.pdf</a></span>   <span><a href='https://github.com/ML-for-Sensor-Data-Western/gmean-mlc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dumindu Tissera, Omar Awadallah, Muhammad Umair Danish, Ayan Sadhu, Katarina Grolinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05721">Any-Class Presence Likelihood for Robust Multi-Label Classification with Abundant Negative Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-label Classification (MLC) assigns an instance to one or more non-exclusive classes. A challenge arises when the dataset contains a large proportion of instances with no assigned class, referred to as negative data, which can overwhelm the learning process and hinder the accurate identification and classification of positive instances. Nevertheless, it is common in MLC applications such as industrial defect detection, agricultural disease identification, and healthcare diagnosis to encounter large amounts of negative data. Assigning a separate negative class to these instances further complicates the learning objective and introduces unnecessary redundancies. To address this challenge, we redesign standard MLC loss functions by deriving a likelihood of any class being present, formulated by a normalized weighted geometric mean of the predicted class probabilities. We introduce a regularization parameter that controls the relative contribution of the absent class probabilities to the any-class presence likelihood in positive instances. The any-class presence likelihood complements the multi-label learning by encouraging the network to become more aware of implicit positive instances and improve the label classification within those positive instances. Experiments on large-scale datasets with negative data: SewerML, modified COCO, and ChestX-ray14, across various networks and base loss functions show that our loss functions consistently improve MLC performance of their standard loss counterparts, achieving gains of up to 6.01 percentage points in F1, 8.06 in F2, and 3.11 in mean average precision, all without additional parameters or computational complexity. Code available at: https://github.com/ML-for-Sensor-Data-Western/gmean-mlc
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2505.24334.pdf' target='_blank'>https://arxiv.org/pdf/2505.24334.pdf</a></span>   <span><a href='https://github.com/intelligolabs/KairosAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Uzair Khan, Franco Fummi, Luigi Capogrosso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24334">KairosAD: A SAM-Based Model for Industrial Anomaly Detection on Embedded Devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era of intelligent manufacturing, anomaly detection has become essential for maintaining quality control on modern production lines. However, while many existing models show promising performance, they are often too large, computationally demanding, and impractical to deploy on resource-constrained embedded devices that can be easily installed on the production lines of Small and Medium Enterprises (SMEs). To bridge this gap, we present KairosAD, a novel supervised approach that uses the power of the Mobile Segment Anything Model (MobileSAM) for image-based anomaly detection. KairosAD has been evaluated on the two well-known industrial anomaly detection datasets, i.e., MVTec-AD and ViSA. The results show that KairosAD requires 78% fewer parameters and boasts a 4x faster inference time compared to the leading state-of-the-art model, while maintaining comparable AUROC performance. We deployed KairosAD on two embedded devices, the NVIDIA Jetson NX, and the NVIDIA Jetson AGX. Finally, KairosAD was successfully installed and tested on the real production line of the Industrial Computer Engineering Laboratory (ICE Lab) at the University of Verona. The code is available at https://github.com/intelligolabs/KairosAD.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2505.17551.pdf' target='_blank'>https://arxiv.org/pdf/2505.17551.pdf</a></span>   <span><a href='https://github.com/cqylunlun/CRAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyu Chen, Huiyuan Luo, Haiming Yao, Wei Luo, Zhen Qu, Chengkan Lv, Zhengtao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.17551">Center-aware Residual Anomaly Synthesis for Multi-class Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection plays a vital role in the inspection of industrial images. Most existing methods require separate models for each category, resulting in multiplied deployment costs. This highlights the challenge of developing a unified model for multi-class anomaly detection. However, the significant increase in inter-class interference leads to severe missed detections. Furthermore, the intra-class overlap between normal and abnormal samples, particularly in synthesis-based methods, cannot be ignored and may lead to over-detection. To tackle these issues, we propose a novel Center-aware Residual Anomaly Synthesis (CRAS) method for multi-class anomaly detection. CRAS leverages center-aware residual learning to couple samples from different categories into a unified center, mitigating the effects of inter-class interference. To further reduce intra-class overlap, CRAS introduces distance-guided anomaly synthesis that adaptively adjusts noise variance based on normal data distribution. Experimental results on diverse datasets and real-world industrial applications demonstrate the superior detection accuracy and competitive inference speed of CRAS. The source code and the newly constructed dataset are publicly available at https://github.com/cqylunlun/CRAS.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2505.09263.pdf' target='_blank'>https://arxiv.org/pdf/2505.09263.pdf</a></span>   <span><a href='https://github.com/gaobb/AnoGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guan Gui, Bin-Bin Gao, Jun Liu, Chengjie Wang, Yunsheng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09263">Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \eg, DRAEM and DseTSeg achieved a 5.8\% and 1.5\% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2504.19524.pdf' target='_blank'>https://arxiv.org/pdf/2504.19524.pdf</a></span>   <span><a href='https://github.com/LilaKen/LR-IAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peijian Zeng, Feiyan Pang, Zhanbo Wang, Aimin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19524">LR-IAD:Mask-Free Industrial Anomaly Detection with Logical Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial Anomaly Detection (IAD) is critical for ensuring product quality by identifying defects. Traditional methods such as feature embedding and reconstruction-based approaches require large datasets and struggle with scalability. Existing vision-language models (VLMs) and Multimodal Large Language Models (MLLMs) address some limitations but rely on mask annotations, leading to high implementation costs and false positives. Additionally, industrial datasets like MVTec-AD and VisA suffer from severe class imbalance, with defect samples constituting only 23.8% and 11.1% of total data respectively. To address these challenges, we propose a reward function that dynamically prioritizes rare defect patterns during training to handle class imbalance. We also introduce a mask-free reasoning framework using Chain of Thought (CoT) and Group Relative Policy Optimization (GRPO) mechanisms, enabling anomaly detection directly from raw images without annotated masks. This approach generates interpretable step-by-step explanations for defect localization. Our method achieves state-of-the-art performance, outperforming prior approaches by 36% in accuracy on MVTec-AD and 16% on VisA. By eliminating mask dependency and reducing costs while providing explainable outputs, this work advances industrial anomaly detection and supports scalable quality control in manufacturing. Code to reproduce the experiment is available at https://github.com/LilaKen/LR-IAD.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2504.12689.pdf' target='_blank'>https://arxiv.org/pdf/2504.12689.pdf</a></span>   <span><a href='https://github.com/Qiqigeww/HSS-IAD-Dataset' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qishan Wang, Shuyong Gao, Junjie Hu, Jiawen Yu, Xuan Tong, You Li, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12689">HSS-IAD: A Heterogeneous Same-Sort Industrial Anomaly Detection Dataset</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-class Unsupervised Anomaly Detection algorithms (MUAD) are receiving increasing attention due to their relatively low deployment costs and improved training efficiency. However, the real-world effectiveness of MUAD methods is questioned due to limitations in current Industrial Anomaly Detection (IAD) datasets. These datasets contain numerous classes that are unlikely to be produced by the same factory and fail to cover multiple structures or appearances. Additionally, the defects do not reflect real-world characteristics. Therefore, we introduce the Heterogeneous Same-Sort Industrial Anomaly Detection (HSS-IAD) dataset, which contains 8,580 images of metallic-like industrial parts and precise anomaly annotations. These parts exhibit variations in structure and appearance, with subtle defects that closely resemble the base materials. We also provide foreground images for synthetic anomaly generation. Finally, we evaluate popular IAD methods on this dataset under multi-class and class-separated settings, demonstrating its potential to bridge the gap between existing datasets and real factory conditions. The dataset is available at https://github.com/Qiqigeww/HSS-IAD-Dataset.
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2504.11055.pdf' target='_blank'>https://arxiv.org/pdf/2504.11055.pdf</a></span>   <span><a href='https://github.com/AlirezaSalehy/Crane' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Salehi, Mohammadreza Salehi, Reshad Hosseini, Cees G. M. Snoek, Makoto Yamada, Mohammad Sabokrou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11055">Crane: Context-Guided Prompt Learning and Attention Refinement for Zero-Shot Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly Detection involves identifying deviations from normal data distributions and is critical in fields such as medical diagnostics and industrial defect detection. Traditional AD methods typically require the availability of normal training samples; however, this assumption is not always feasible. Recently, the rich pretraining knowledge of CLIP has shown promising zero-shot generalization in detecting anomalies without the need for training samples from target domains. However, CLIP's coarse-grained image-text alignment limits localization and detection performance for fine-grained anomalies due to: (1) spatial misalignment, and (2) the limited sensitivity of global features to local anomalous patterns. In this paper, we propose Crane which tackles both problems. First, we introduce a correlation-based attention module to retain spatial alignment more accurately. Second, to boost the model's awareness of fine-grained anomalies, we condition the learnable prompts of the text encoder on image context extracted from the vision encoder and perform a local-to-global representation fusion. Moreover, our method can incorporate vision foundation models such as DINOv2 to further enhance spatial understanding and localization. The key insight of Crane is to balance learnable adaptations for modeling anomalous concepts with non-learnable adaptations that preserve and exploit generalized pretrained knowledge, thereby minimizing in-domain overfitting and maximizing performance on unseen domains. Extensive evaluation across 14 diverse industrial and medical datasets demonstrates that Crane consistently improves the state-of-the-art ZSAD from 2% to 28%, at both image and pixel levels, while remaining competitive in inference speed. The code is available at https://github.com/AlirezaSalehy/Crane.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2503.23451.pdf' target='_blank'>https://arxiv.org/pdf/2503.23451.pdf</a></span>   <span><a href='https://github.com/abc-125/viad-benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aimira Baitieva, Yacine Bouaouni, Alexandre Briot, Dick Ameln, Souhaiel Khalfaoui, Samet Akcay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23451">Beyond Academic Benchmarks: Critical Analysis and Best Practices for Visual Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection (AD) is essential for automating visual inspection in manufacturing. This field of computer vision is rapidly evolving, with increasing attention towards real-world applications. Meanwhile, popular datasets are typically produced in controlled lab environments with artificially created defects, unable to capture the diversity of real production conditions. New methods often fail in production settings, showing significant performance degradation or requiring impractical computational resources. This disconnect between academic results and industrial viability threatens to misdirect visual anomaly detection research. This paper makes three key contributions: (1) we demonstrate the importance of real-world datasets and establish benchmarks using actual production data, (2) we provide a fair comparison of existing SOTA methods across diverse tasks by utilizing metrics that are valuable for practical applications, and (3) we present a comprehensive analysis of recent advancements in this field by discussing important challenges and new perspectives for bridging the academia-industry gap. The code is publicly available at https://github.com/abc-125/viad-benchmark
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2503.13184.pdf' target='_blank'>https://arxiv.org/pdf/2503.13184.pdf</a></span>   <span><a href='https://github.com/tzjtatata/Triad' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanze Li, Shihao Yuan, Haolin Wang, Qizhang Li, Ming Liu, Chen Xu, Guangming Shi, Wangmeng Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13184">Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided Visual Tokenizer and Manufacturing Process</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although recent methods have tried to introduce large multimodal models (LMMs) into industrial anomaly detection (IAD), their generalization in the IAD field is far inferior to that for general purposes. We summarize the main reasons for this gap into two aspects. On one hand, general-purpose LMMs lack cognition of defects in the visual modality, thereby failing to sufficiently focus on defect areas. Therefore, we propose to modify the AnyRes structure of the LLaVA model, providing the potential anomalous areas identified by existing IAD models to the LMMs. On the other hand, existing methods mainly focus on identifying defects by learning defect patterns or comparing with normal samples, yet they fall short of understanding the causes of these defects. Considering that the generation of defects is closely related to the manufacturing process, we propose a manufacturing-driven IAD paradigm. An instruction-tuning dataset for IAD (InstructIAD) and a data organization approach for Chain-of-Thought with manufacturing (CoT-M) are designed to leverage the manufacturing process for IAD. Based on the above two modifications, we present Triad, a novel LMM-based method incorporating an expert-guided region-of-interest tokenizer and manufacturing process for industrial anomaly detection. Extensive experiments show that our Triad not only demonstrates competitive performance against current LMMs but also achieves further improved accuracy when equipped with manufacturing processes. Source code, training data, and pre-trained models will be publicly available at https://github.com/tzjtatata/Triad.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2502.13280.pdf' target='_blank'>https://arxiv.org/pdf/2502.13280.pdf</a></span>   <span><a href='https://github.com/swyoon/value-gradient-sampler/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangwoong Yoon, Himchan Hwang, Hyeokju Jeong, Dong Kyu Shin, Che-Sang Park, Sehee Kweon, Frank Chongwoo Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13280">Value Gradient Sampler: Sampling as Sequential Decision Making</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose the Value Gradient Sampler (VGS), a trainable sampler based on the interpretation of sampling as discrete-time sequential decision-making. VGS generates samples from a given unnormalized density (i.e., energy) by drifting and diffusing randomly initialized particles. In VGS, finding the optimal drift is equivalent to solving an optimal control problem where the cost is the upper bound of the KL divergence between the target density and the samples. We employ value-based dynamic programming to solve this optimal control problem, which gives the gradient of the value function as the optimal drift vector. The connection to sequential decision making allows VGS to leverage extensively studied techniques in reinforcement learning, making VGS a fast, adaptive, and accurate sampler that achieves competitive results in various sampling benchmarks. Furthermore, VGS can replace MCMC in contrastive divergence training of energy-based models. We demonstrate the effectiveness of VGS in training accurate energy-based models in industrial anomaly detection applications.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2502.05761.pdf' target='_blank'>https://arxiv.org/pdf/2502.05761.pdf</a></span>   <span><a href='https://github.com/EnquanYang2022/3CAD' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/EnquanYang2022/3CAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Enquan Yang, Peng Xing, Hanyang Sun, Wenbo Guo, Yuanwei Ma, Zechao Li, Dan Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05761">3CAD: A Large-Scale Real-World 3C Product Dataset for Unsupervised Anomaly</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection achieves progress thanks to datasets such as MVTec-AD and VisA. However, they suffer from limitations in terms of the number of defect samples, types of defects, and availability of real-world scenes. These constraints inhibit researchers from further exploring the performance of industrial detection with higher accuracy. To this end, we propose a new large-scale anomaly detection dataset called 3CAD, which is derived from real 3C production lines. Specifically, the proposed 3CAD includes eight different types of manufactured parts, totaling 27,039 high-resolution images labeled with pixel-level anomalies. The key features of 3CAD are that it covers anomalous regions of different sizes, multiple anomaly types, and the possibility of multiple anomalous regions and multiple anomaly types per anomaly image. This is the largest and first anomaly detection dataset dedicated to 3C product quality control for community exploration and development. Meanwhile, we introduce a simple yet effective framework for unsupervised anomaly detection: a Coarse-to-Fine detection paradigm with Recovery Guidance (CFRG). To detect small defect anomalies, the proposed CFRG utilizes a coarse-to-fine detection paradigm. Specifically, we utilize a heterogeneous distillation model for coarse localization and then fine localization through a segmentation model. In addition, to better capture normal patterns, we introduce recovery features as guidance. Finally, we report the results of our CFRG framework and popular anomaly detection methods on the 3CAD dataset, demonstrating strong competitiveness and providing a highly challenging benchmark to promote the development of the anomaly detection field. Data and code are available: https://github.com/EnquanYang2022/3CAD.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2501.15434.pdf' target='_blank'>https://arxiv.org/pdf/2501.15434.pdf</a></span>   <span><a href='https://github.com/rohban-lab/COBRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hossein Mirzaei, Mojtaba Nafez, Jafar Habibi, Mohammad Sabokrou, Mohammad Hossein Rohban
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15434">Mitigating Spurious Negative Pairs for Robust Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite significant progress in Anomaly Detection (AD), the robustness of existing detection methods against adversarial attacks remains a challenge, compromising their reliability in critical real-world applications such as autonomous driving. This issue primarily arises from the AD setup, which assumes that training data is limited to a group of unlabeled normal samples, making the detectors vulnerable to adversarial anomaly samples during testing. Additionally, implementing adversarial training as a safeguard encounters difficulties, such as formulating an effective objective function without access to labels. An ideal objective function for adversarial training in AD should promote strong perturbations both within and between the normal and anomaly groups to maximize margin between normal and anomaly distribution. To address these issues, we first propose crafting a pseudo-anomaly group derived from normal group samples. Then, we demonstrate that adversarial training with contrastive loss could serve as an ideal objective function, as it creates both inter- and intra-group perturbations. However, we notice that spurious negative pairs compromise the conventional contrastive loss to achieve robust AD. Spurious negative pairs are those that should be closely mapped but are erroneously separated. These pairs introduce noise and misguide the direction of inter-group adversarial perturbations. To overcome the effect of spurious negative pairs, we define opposite pairs and adversarially pull them apart to strengthen inter-group perturbations. Experimental results demonstrate our superior performance in both clean and adversarial scenarios, with a 26.1% improvement in robust detection across various challenging benchmark datasets. The implementation of our work is available at: https://github.com/rohban-lab/COBRA.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2412.20870.pdf' target='_blank'>https://arxiv.org/pdf/2412.20870.pdf</a></span>   <span><a href='https://github.com/TencentYoutuResearch/AnomalyDetection-SoftPatch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengjie Wang, Xi Jiang, Bin-Bin Gao, Zhenye Gan, Yong Liu, Feng Zheng, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20870">SoftPatch+: Fully Unsupervised Anomaly Classification and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although mainstream unsupervised anomaly detection (AD) (including image-level classification and pixel-level segmentation)algorithms perform well in academic datasets, their performance is limited in practical application due to the ideal experimental setting of clean training data. Training with noisy data is an inevitable problem in real-world anomaly detection but is seldom discussed. This paper is the first to consider fully unsupervised industrial anomaly detection (i.e., unsupervised AD with noisy data). To solve this problem, we proposed memory-based unsupervised AD methods, SoftPatch and SoftPatch+, which efficiently denoise the data at the patch level. Noise discriminators are utilized to generate outlier scores for patch-level noise elimination before coreset construction. The scores are then stored in the memory bank to soften the anomaly detection boundary. Compared with existing methods, SoftPatch maintains a strong modeling ability of normal data and alleviates the overconfidence problem in coreset, and SoftPatch+ has more robust performance which is articularly useful in real-world industrial inspection scenarios with high levels of noise (from 10% to 40%). Comprehensive experiments conducted in diverse noise scenarios demonstrate that both SoftPatch and SoftPatch+ outperform the state-of-the-art AD methods on the MVTecAD, ViSA, and BTAD benchmarks. Furthermore, the performance of SoftPatch and SoftPatch+ is comparable to that of the noise-free methods in conventional unsupervised AD setting. The code of the proposed methods can be found at https://github.com/TencentYoutuResearch/AnomalyDetection-SoftPatch.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2412.17458.pdf' target='_blank'>https://arxiv.org/pdf/2412.17458.pdf</a></span>   <span><a href='https://github.com/cqylunlun/PBAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyu Chen, Huiyuan Luo, Han Gao, Chengkan Lv, Zhengtao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.17458">Progressive Boundary Guided Anomaly Synthesis for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised anomaly detection methods can identify surface defects in industrial images by leveraging only normal samples for training. Due to the risk of overfitting when learning from a single class, anomaly synthesis strategies are introduced to enhance detection capability by generating artificial anomalies. However, existing strategies heavily rely on anomalous textures from auxiliary datasets. Moreover, their limitations in the coverage and directionality of anomaly synthesis may result in a failure to capture useful information and lead to significant redundancy. To address these issues, we propose a novel Progressive Boundary-guided Anomaly Synthesis (PBAS) strategy, which can directionally synthesize crucial feature-level anomalies without auxiliary textures. It consists of three core components: Approximate Boundary Learning (ABL), Anomaly Feature Synthesis (AFS), and Refined Boundary Optimization (RBO). To make the distribution of normal samples more compact, ABL first learns an approximate decision boundary by center constraint, which improves the center initialization through feature alignment. AFS then directionally synthesizes anomalies with more flexible scales guided by the hypersphere distribution of normal features. Since the boundary is so loose that it may contain real anomalies, RBO refines the decision boundary through the binary classification of artificial anomalies and normal features. Experimental results show that our method achieves state-of-the-art performance and the fastest detection speed on three widely used industrial datasets, including MVTec AD, VisA, and MPDD. The code will be available at: https://github.com/cqylunlun/PBAS.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2412.11802.pdf' target='_blank'>https://arxiv.org/pdf/2412.11802.pdf</a></span>   <span><a href='https://github.com/luow23/AMI-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Luo, Haiming Yao, Wenyong Yu, Zhengyong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.11802">AMI-Net: Adaptive Mask Inpainting Network for Industrial Anomaly Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised visual anomaly detection is crucial for enhancing industrial production quality and efficiency. Among unsupervised methods, reconstruction approaches are popular due to their simplicity and effectiveness. The key aspect of reconstruction methods lies in the restoration of anomalous regions, which current methods have not satisfactorily achieved. To tackle this issue, we introduce a novel \uline{A}daptive \uline{M}ask \uline{I}npainting \uline{Net}work (AMI-Net) from the perspective of adaptive mask-inpainting. In contrast to traditional reconstruction methods that treat non-semantic image pixels as targets, our method uses a pre-trained network to extract multi-scale semantic features as reconstruction targets. Given the multiscale nature of industrial defects, we incorporate a training strategy involving random positional and quantitative masking. Moreover, we propose an innovative adaptive mask generator capable of generating adaptive masks that effectively mask anomalous regions while preserving normal regions. In this manner, the model can leverage the visible normal global contextual information to restore the masked anomalous regions, thereby effectively suppressing the reconstruction of defects. Extensive experimental results on the MVTec AD and BTAD industrial datasets validate the effectiveness of the proposed method. Additionally, AMI-Net exhibits exceptional real-time performance, striking a favorable balance between detection accuracy and speed, rendering it highly suitable for industrial applications. Code is available at: https://github.com/luow23/AMI-Net
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2412.08949.pdf' target='_blank'>https://arxiv.org/pdf/2412.08949.pdf</a></span>   <span><a href='https://github.com/hito2448/TRD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyue Liu, Jianyuan Wang, Biao Leng, Shuo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08949">Tuned Reverse Distillation: Enhancing Multimodal Industrial Anomaly Detection with Crossmodal Tuners</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge distillation (KD) has been widely studied in unsupervised image Anomaly Detection (AD), but its application to unsupervised multimodal AD remains underexplored. Existing KD-based methods for multimodal AD that use fused multimodal features to obtain teacher representations face challenges. Anomalies that only exist in one modality may not be effectively captured in the fused teacher features, leading to detection failures. Besides, these methods do not fully leverage the rich intra- and inter-modality information that are critical for effective anomaly detection. In this paper, we propose Tuned Reverse Distillation (TRD) based on Multi-branch design to realize Multimodal Industrial AD. By assigning independent branches to each modality, our method enables finer detection of anomalies within each modality. Furthermore, we enhance the interaction between modalities during the distillation process by designing two Crossmodal Tuners including Crossmodal Filter and Amplifier. With the idea of crossmodal mapping, the student network is allowed to better learn normal features while anomalies in all modalities are ensured to be effectively detected. Experimental verifications on multimodal AD datasets demonstrate that our method achieves state-of-the-art performance in multimodal anomaly detection and localization. Code is available at https://github.com/hito2448/TRD.
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2412.03969.pdf' target='_blank'>https://arxiv.org/pdf/2412.03969.pdf</a></span>   <span><a href='https://github.com/Jay-zzcoder/HD-YOLO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuo Zuo, Jiahao Dong, Yue Gao, Zongze Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03969">HyperDefect-YOLO: Enhance YOLO with HyperGraph Computation for Industrial Defect Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the manufacturing industry, defect detection is an essential but challenging task aiming to detect defects generated in the process of production. Though traditional YOLO models presents a good performance in defect detection, they still have limitations in capturing high-order feature interrelationships, which hurdles defect detection in the complex scenarios and across the scales. To this end, we introduce hypergraph computation into YOLO framework, dubbed HyperDefect-YOLO (HD-YOLO), to improve representative ability and semantic exploitation. HD-YOLO consists of Defect Aware Module (DAM) and Mixed Graph Network (MGNet) in the backbone, which specialize for perception and extraction of defect features. To effectively aggregate multi-scale features, we propose HyperGraph Aggregation Network (HGANet) which combines hypergraph and attention mechanism to aggregate multi-scale features. Cross-Scale Fusion (CSF) is proposed to adaptively fuse and handle features instead of simple concatenation and convolution. Finally, we propose Semantic Aware Module (SAM) in the neck to enhance semantic exploitation for accurately localizing defects with different sizes in the disturbed background. HD-YOLO undergoes rigorous evaluation on public HRIPCB and NEU-DET datasets with significant improvements compared to state-of-the-art methods. We also evaluate HD-YOLO on self-built MINILED dataset collected in real industrial scenarios to demonstrate the effectiveness of the proposed method. The source codes are at https://github.com/Jay-zzcoder/HD-YOLO.
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2410.16255.pdf' target='_blank'>https://arxiv.org/pdf/2410.16255.pdf</a></span>   <span><a href='https://github.com/sukanyapatra1997/ULSAD-2024.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sukanya Patra, Souhaib Ben Taieb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16255">Revisiting Deep Feature Reconstruction for Logical and Structural Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection is crucial for quality control and predictive maintenance, but it presents challenges due to limited training data, diverse anomaly types, and external factors that alter object appearances. Existing methods commonly detect structural anomalies, such as dents and scratches, by leveraging multi-scale features from image patches extracted through deep pre-trained networks. However, significant memory and computational demands often limit their practical application. Additionally, detecting logical anomalies-such as images with missing or excess elements-requires an understanding of spatial relationships that traditional patch-based methods fail to capture. In this work, we address these limitations by focusing on Deep Feature Reconstruction (DFR), a memory- and compute-efficient approach for detecting structural anomalies. We further enhance DFR into a unified framework, called ULSAD, which is capable of detecting both structural and logical anomalies. Specifically, we refine the DFR training objective to improve performance in structural anomaly detection, while introducing an attention-based loss mechanism using a global autoencoder-like network to handle logical anomaly detection. Our empirical evaluation across five benchmark datasets demonstrates the performance of ULSAD in detecting and localizing both structural and logical anomalies, outperforming eight state-of-the-art methods. An extensive ablation study further highlights the contribution of each component to the overall performance improvement. Our code is available at https://github.com/sukanyapatra1997/ULSAD-2024.git
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2410.09821.pdf' target='_blank'>https://arxiv.org/pdf/2410.09821.pdf</a></span>   <span><a href='https://github.com/SunnierLee/DAS3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kecen Li, Bingquan Dai, Jingjing Fu, Xinwen Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09821">DAS3D: Dual-modality Anomaly Synthesis for 3D Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing anomaly samples has proven to be an effective strategy for self-supervised 2D industrial anomaly detection. However, this approach has been rarely explored in multi-modality anomaly detection, particularly involving 3D and RGB images. In this paper, we propose a novel dual-modality augmentation method for 3D anomaly synthesis, which is simple and capable of mimicking the characteristics of 3D defects. Incorporating with our anomaly synthesis method, we introduce a reconstruction-based discriminative anomaly detection network, in which a dual-modal discriminator is employed to fuse the original and reconstructed embedding of two modalities for anomaly detection. Additionally, we design an augmentation dropout mechanism to enhance the generalizability of the discriminator. Extensive experiments show that our method outperforms the state-of-the-art methods on detection precision and achieves competitive segmentation performance on both MVTec 3D-AD and Eyescandies datasets.
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2410.09453.pdf' target='_blank'>https://arxiv.org/pdf/2410.09453.pdf</a></span>   <span><a href='https://github.com/jam-cc/MMAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Jiang, Jian Li, Hanqiu Deng, Yong Liu, Bin-Bin Gao, Yifeng Zhou, Jialin Li, Chengjie Wang, Feng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09453">MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the field of industrial inspection, Multimodal Large Language Models (MLLMs) have a high potential to renew the paradigms in practical applications due to their robust language capabilities and generalization abilities. However, despite their impressive problem-solving skills in many domains, MLLMs' ability in industrial anomaly detection has not been systematically studied. To bridge this gap, we present MMAD, the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. We defined seven key subtasks of MLLMs in industrial inspection and designed a novel pipeline to generate the MMAD dataset with 39,672 questions for 8,366 industrial images. With MMAD, we have conducted a comprehensive, quantitative evaluation of various state-of-the-art MLLMs. The commercial models performed the best, with the average accuracy of GPT-4o models reaching 74.9%. However, this result falls far short of industrial requirements. Our analysis reveals that current MLLMs still have significant room for improvement in answering questions related to industrial anomalies and defects. We further explore two training-free performance enhancement strategies to help models improve in industrial scenarios, highlighting their promising potential for future research.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2410.00713.pdf' target='_blank'>https://arxiv.org/pdf/2410.00713.pdf</a></span>   <span><a href='https://github.com/kaichen-z/RAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaichen Zhou, Yang Cao, Taewhan Kim, Hao Zhao, Hao Dong, Kai Ming Ting, Ye Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00713">RAD: A Dataset and Benchmark for Real-Life Anomaly Detection with Robotic Observations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in industrial anomaly detection have been hindered by the lack of realistic datasets that accurately represent real-world conditions. Existing algorithms are often developed and evaluated using idealized datasets, which deviate significantly from real-life scenarios characterized by environmental noise and data corruption such as fluctuating lighting conditions, variable object poses, and unstable camera positions. To address this gap, we introduce the Realistic Anomaly Detection (RAD) dataset, the first multi-view RGB-based anomaly detection dataset specifically collected using a real robot arm, providing unique and realistic data scenarios. RAD comprises 4765 images across 13 categories and 4 defect types, collected from more than 50 viewpoints, providing a comprehensive and realistic benchmark. This multi-viewpoint setup mirrors real-world conditions where anomalies may not be detectable from every perspective. Moreover, by sampling varying numbers of views, the algorithm's performance can be comprehensively evaluated across different viewpoints. This approach enhances the thoroughness of performance assessment and helps improve the algorithm's robustness. Besides, to support 3D multi-view reconstruction algorithms, we propose a data augmentation method to improve the accuracy of pose estimation and facilitate the reconstruction of 3D point clouds. We systematically evaluate state-of-the-art RGB-based and point cloud-based models using RAD, identifying limitations and future research directions. The code and dataset could found at https://github.com/kaichen-z/RAD
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2409.00556.pdf' target='_blank'>https://arxiv.org/pdf/2409.00556.pdf</a></span>   <span><a href='https://github.com/BMVC-FADE/BMVC-FADE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanwei Li, Elizaveta Ivanova, Martins Bruveris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00556">FADE: Few-shot/zero-shot Anomaly Detection Engine using Large Vision-Language Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic image anomaly detection is important for quality inspection in the manufacturing industry. The usual unsupervised anomaly detection approach is to train a model for each object class using a dataset of normal samples. However, a more realistic problem is zero-/few-shot anomaly detection where zero or only a few normal samples are available. This makes the training of object-specific models challenging. Recently, large foundation vision-language models have shown strong zero-shot performance in various downstream tasks. While these models have learned complex relationships between vision and language, they are not specifically designed for the tasks of anomaly detection. In this paper, we propose the Few-shot/zero-shot Anomaly Detection Engine (FADE) which leverages the vision-language CLIP model and adjusts it for the purpose of industrial anomaly detection. Specifically, we improve language-guided anomaly segmentation 1) by adapting CLIP to extract multi-scale image patch embeddings that are better aligned with language and 2) by automatically generating an ensemble of text prompts related to industrial anomaly detection. 3) We use additional vision-based guidance from the query and reference images to further improve both zero-shot and few-shot anomaly detection. On the MVTec-AD (and VisA) dataset, FADE outperforms other state-of-the-art methods in anomaly segmentation with pixel-AUROC of 89.6% (91.5%) in zero-shot and 95.4% (97.5%) in 1-normal-shot. Code is available at https://github.com/BMVC-FADE/BMVC-FADE.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2407.21351.pdf' target='_blank'>https://arxiv.org/pdf/2407.21351.pdf</a></span>   <span><a href='https://github.com/zhangzilongc/SOFS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilong Zhang, Chang Niu, Zhibin Zhao, Xingwu Zhang, Xuefeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21351">Small Object Few-shot Segmentation for Vision-based Industrial Inspection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-based industrial inspection (VII) aims to locate defects quickly and accurately. Supervised learning under a close-set setting and industrial anomaly detection, as two common paradigms in VII, face different problems in practical applications. The former is that various and sufficient defects are difficult to obtain, while the latter is that specific defects cannot be located. To solve these problems, in this paper, we focus on the few-shot semantic segmentation (FSS) method, which can locate unseen defects conditioned on a few annotations without retraining. Compared to common objects in natural images, the defects in VII are small. This brings two problems to current FSS methods: 1 distortion of target semantics and 2 many false positives for backgrounds. To alleviate these problems, we propose a small object few-shot segmentation (SOFS) model. The key idea for alleviating 1 is to avoid the resizing of the original image and correctly indicate the intensity of target semantics. SOFS achieves this idea via the non-resizing procedure and the prototype intensity downsampling of support annotations. To alleviate 2, we design an abnormal prior map in SOFS to guide the model to reduce false positives and propose a mixed normal Dice loss to preferentially prevent the model from predicting false positives. SOFS can achieve FSS and few-shot anomaly detection determined by support masks. Diverse experiments substantiate the superior performance of SOFS. Code is available at https://github.com/zhangzilongc/SOFS.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2407.09359.pdf' target='_blank'>https://arxiv.org/pdf/2407.09359.pdf</a></span>   <span><a href='https://github.com/cqylunlun/GLASS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiyu Chen, Huiyuan Luo, Chengkan Lv, Zhengtao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.09359">A Unified Anomaly Synthesis Strategy with Gradient Ascent for Industrial Anomaly Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly synthesis strategies can effectively enhance unsupervised anomaly detection. However, existing strategies have limitations in the coverage and controllability of anomaly synthesis, particularly for weak defects that are very similar to normal regions. In this paper, we propose Global and Local Anomaly co-Synthesis Strategy (GLASS), a novel unified framework designed to synthesize a broader coverage of anomalies under the manifold and hypersphere distribution constraints of Global Anomaly Synthesis (GAS) at the feature level and Local Anomaly Synthesis (LAS) at the image level. Our method synthesizes near-in-distribution anomalies in a controllable way using Gaussian noise guided by gradient ascent and truncated projection. GLASS achieves state-of-the-art results on the MVTec AD (detection AUROC of 99.9\%), VisA, and MPDD datasets and excels in weak defect detection. The effectiveness and efficiency have been further validated in industrial applications for woven fabric defect detection. The code and dataset are available at: \url{https://github.com/cqylunlun/GLASS}.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2405.09933.pdf' target='_blank'>https://arxiv.org/pdf/2405.09933.pdf</a></span>   <span><a href='https://github.com/WangFengJiee/MiniMaxAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengjie Wang, Chengming Liu, Lei Shi, Pang Haibo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09933">MiniMaxAD: A Lightweight Autoencoder for Feature-Rich Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous industrial anomaly detection methods often struggle to handle the extensive diversity in training sets, particularly when they contain stylistically diverse and feature-rich samples, which we categorize as feature-rich anomaly detection datasets (FRADs). This challenge is evident in applications such as multi-view and multi-class scenarios. To address this challenge, we developed MiniMaxAD, a efficient autoencoder designed to efficiently compress and memorize extensive information from normal images. Our model employs a technique that enhances feature diversity, thereby increasing the effective capacity of the network. It also utilizes large kernel convolution to extract highly abstract patterns, which contribute to efficient and compact feature embedding. Moreover, we introduce an Adaptive Contraction Hard Mining Loss (ADCLoss), specifically tailored to FRADs. In our methodology, any dataset can be unified under the framework of feature-rich anomaly detection, in a way that the benefits far outweigh the drawbacks. Our approach has achieved state-of-the-art performance in multiple challenging benchmarks. Code is available at: \href{https://github.com/WangFengJiee/MiniMaxAD}{https://github.com/WangFengJiee/MiniMaxAD}
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2402.19330.pdf' target='_blank'>https://arxiv.org/pdf/2402.19330.pdf</a></span>   <span><a href='https://github.com/GrandpaXun242/AdaBLDM.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxi Li, Zhengxun Zhang, Hao Chen, Lin Wu, Bo Li, Deyin Liu, Mingwen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19330">A Novel Approach to Industrial Defect Generation through Blended Latent Diffusion Model with Online Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effectively addressing the challenge of industrial Anomaly Detection (AD) necessitates an ample supply of defective samples, a constraint often hindered by their scarcity in industrial contexts. This paper introduces a novel algorithm designed to augment defective samples, thereby enhancing AD performance. The proposed method tailors the blended latent diffusion model for defect sample generation, employing a diffusion model to generate defective samples in the latent space. A feature editing process, controlled by a ``trimap" mask and text prompts, refines the generated samples. The image generation inference process is structured into three stages: a free diffusion stage, an editing diffusion stage, and an online decoder adaptation stage. This sophisticated inference strategy yields high-quality synthetic defective samples with diverse pattern variations, leading to significantly improved AD accuracies based on the augmented training set. Specifically, on the widely recognized MVTec AD dataset, the proposed method elevates the state-of-the-art (SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD metrics AP, IAP, and IAP90, respectively. The implementation code of this work can be found at the GitHub repository https://github.com/GrandpaXun242/AdaBLDM.git
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2310.19070.pdf' target='_blank'>https://arxiv.org/pdf/2310.19070.pdf</a></span>   <span><a href='https://github.com/tzjtatata/Myriad' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanze Li, Haolin Wang, Shihao Yuan, Ming Liu, Debin Zhao, Yiwen Guo, Chen Xu, Guangming Shi, Wangmeng Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.19070">Myriad: Large Multimodal Model by Applying Vision Experts for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the training configuration, traditional industrial anomaly detection (IAD) methods have to train a specific model for each deployment scenario, which is insufficient to meet the requirements of modern design and manufacturing. On the contrary, large multimodal models~(LMMs) have shown eminent generalization ability on various vision tasks, and their perception and comprehension capabilities imply the potential of applying LMMs on IAD tasks. However, we observe that even though the LMMs have abundant knowledge about industrial anomaly detection in the textual domain, the LMMs are unable to leverage the knowledge due to the modality gap between textual and visual domains. To stimulate the relevant knowledge in LMMs and adapt the LMMs towards anomaly detection tasks, we introduce existing IAD methods as vision experts and present a novel large multimodal model applying vision experts for industrial anomaly detection~(abbreviated to {Myriad}). Specifically, we utilize the anomaly map generated by the vision experts as guidance for LMMs, such that the vision model is guided to pay more attention to anomalous regions. Then, the visual features are modulated via an adapter to fit the anomaly detection tasks, which are fed into the language model together with the vision expert guidance and human instructions to generate the final outputs. Extensive experiments are applied on MVTec-AD, VisA, and PCB Bank benchmarks demonstrate that our proposed method not only performs favorably against state-of-the-art methods, but also inherits the flexibility and instruction-following ability of LMMs in the field of IAD. Source code and pre-trained models are publicly available at \url{https://github.com/tzjtatata/Myriad}.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2310.02576.pdf' target='_blank'>https://arxiv.org/pdf/2310.02576.pdf</a></span>   <span><a href='https://github.com/98chao/ProtoAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Huang, Zhao Kang, Hong Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02576">A Prototype-Based Neural Network for Image Anomaly Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image anomaly detection and localization perform not only image-level anomaly classification but also locate pixel-level anomaly regions. Recently, it has received much research attention due to its wide application in various fields. This paper proposes ProtoAD, a prototype-based neural network for image anomaly detection and localization. First, the patch features of normal images are extracted by a deep network pre-trained on nature images. Then, the prototypes of the normal patch features are learned by non-parametric clustering. Finally, we construct an image anomaly localization network (ProtoAD) by appending the feature extraction network with $L2$ feature normalization, a $1\times1$ convolutional layer, a channel max-pooling, and a subtraction operation. We use the prototypes as the kernels of the $1\times1$ convolutional layer; therefore, our neural network does not need a training phase and can conduct anomaly detection and localization in an end-to-end manner. Extensive experiments on two challenging industrial anomaly detection datasets, MVTec AD and BTAD, demonstrate that ProtoAD achieves competitive performance compared to the state-of-the-art methods with a higher inference speed. The source code is available at: https://github.com/98chao/ProtoAD.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2309.13226.pdf' target='_blank'>https://arxiv.org/pdf/2309.13226.pdf</a></span>   <span><a href='https://github.com/M-3LAB/Real3D-AD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Liu, Guoyang Xie, Ruitao Chen, Xinpeng Li, Jinbao Wang, Yong Liu, Chengjie Wang, Feng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13226">Real3D-AD: A Dataset of Point Cloud Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-precision point cloud anomaly detection is the gold standard for identifying the defects of advancing machining and precision manufacturing. Despite some methodological advances in this area, the scarcity of datasets and the lack of a systematic benchmark hinder its development. We introduce Real3D-AD, a challenging high-precision point cloud anomaly detection dataset, addressing the limitations in the field. With 1,254 high-resolution 3D items from forty thousand to millions of points for each item, Real3D-AD is the largest dataset for high-precision 3D industrial anomaly detection to date. Real3D-AD surpasses existing 3D anomaly detection datasets available regarding point cloud resolution (0.0010mm-0.0015mm), 360 degree coverage and perfect prototype. Additionally, we present a comprehensive benchmark for Real3D-AD, revealing the absence of baseline methods for high-precision point cloud anomaly detection. To address this, we propose Reg3D-AD, a registration-based 3D anomaly detection method incorporating a novel feature memory bank that preserves local and global representations. Extensive experiments on the Real3D-AD dataset highlight the effectiveness of Reg3D-AD. For reproducibility and accessibility, we provide the Real3D-AD dataset, benchmark source code, and Reg3D-AD on our website:https://github.com/M-3LAB/Real3D-AD.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2309.07068.pdf' target='_blank'>https://arxiv.org/pdf/2309.07068.pdf</a></span>   <span><a href='https://github.com/liutongkun/FAIR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongkun Liu, Bing Li, Xiao Du, Bingke Jiang, Leqi Geng, Feiyang Wang, Zhuo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.07068">FAIR: Frequency-aware Image Restoration for Industrial Visual Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image reconstruction-based anomaly detection models are widely explored in industrial visual inspection. However, existing models usually suffer from the trade-off between normal reconstruction fidelity and abnormal reconstruction distinguishability, which damages the performance. In this paper, we find that the above trade-off can be better mitigated by leveraging the distinct frequency biases between normal and abnormal reconstruction errors. To this end, we propose Frequency-aware Image Restoration (FAIR), a novel self-supervised image restoration task that restores images from their high-frequency components. It enables precise reconstruction of normal patterns while mitigating unfavorable generalization to anomalies. Using only a simple vanilla UNet, FAIR achieves state-of-the-art performance with higher efficiency on various defect detection datasets. Code: https://github.com/liutongkun/FAIR.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2308.15366.pdf' target='_blank'>https://arxiv.org/pdf/2308.15366.pdf</a></span>   <span><a href='https://github.com/CASIA-IVA-Lab/AnomalyGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.15366">AnomalyGPT: Detecting Industrial Anomalies Using Large Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision-Language Models (LVLMs) such as MiniGPT-4 and LLaVA have demonstrated the capability of understanding images and achieved remarkable performance in various visual tasks. Despite their strong abilities in recognizing common objects due to extensive training datasets, they lack specific domain knowledge and have a weaker understanding of localized details within objects, which hinders their effectiveness in the Industrial Anomaly Detection (IAD) task. On the other hand, most existing IAD methods only provide anomaly scores and necessitate the manual setting of thresholds to distinguish between normal and abnormal samples, which restricts their practical implementation. In this paper, we explore the utilization of LVLM to address the IAD problem and propose AnomalyGPT, a novel IAD approach based on LVLM. We generate training data by simulating anomalous images and producing corresponding textual descriptions for each image. We also employ an image decoder to provide fine-grained semantic and design a prompt learner to fine-tune the LVLM using prompt embeddings. Our AnomalyGPT eliminates the need for manual threshold adjustments, thus directly assesses the presence and locations of anomalies. Additionally, AnomalyGPT supports multi-turn dialogues and exhibits impressive few-shot in-context learning capabilities. With only one normal shot, AnomalyGPT achieves the state-of-the-art performance with an accuracy of 86.1%, an image-level AUC of 94.1%, and a pixel-level AUC of 95.3% on the MVTec-AD dataset. Code is available at https://github.com/CASIA-IVA-Lab/AnomalyGPT.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2308.12577.pdf' target='_blank'>https://arxiv.org/pdf/2308.12577.pdf</a></span>   <span><a href='https://github.com/ShuaiLYU/REB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Lyu, Dongmei Mo, Waikeung Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12577">REB: Reducing Biases in Representation for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing representation-based methods usually conduct industrial anomaly detection in two stages: obtain feature representations with a pre-trained model and perform distance measures for anomaly detection. Among them, K-nearest neighbor (KNN) retrieval-based anomaly detection methods show promising results. However, the features are not fully exploited as these methods ignore domain bias of pre-trained models and the difference of local density in feature space, which limits the detection performance. In this paper, we propose Reducing Biases (REB) in representation by considering the domain bias and building a self-supervised learning task for better domain adaption with a defect generation strategy (DefectMaker) that ensures a strong diversity in the synthetic defects. Additionally, we propose a local-density KNN (LDKNN) to reduce the local density bias in the feature space and obtain effective anomaly detection. The proposed REB method achieves a promising result of 99.5\% Im.AUROC on the widely used MVTec AD, with smaller backbone networks such as Vgg11 and Resnet18. The method also achieves an impressive 88.8\% Im.AUROC on the MVTec LOCO AD dataset and a remarkable 96.0\% on the BTAD dataset, outperforming other representation-based approaches. These results indicate the effectiveness and efficiency of REB for practical industrial applications. Code:https://github.com/ShuaiLYU/REB.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2305.08509.pdf' target='_blank'>https://arxiv.org/pdf/2305.08509.pdf</a></span>   <span><a href='https://github.com/liutongkun/ComAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongkun Liu, Bing Li, Xiao Du, Bingke Jiang, Xiao Jin, Liuyi Jin, Zhuo Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08509">Component-aware anomaly detection framework for adjustable and logical industrial visual inspection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial visual inspection aims at detecting surface defects in products during the manufacturing process. Although existing anomaly detection models have shown great performance on many public benchmarks, their limited adjustability and ability to detect logical anomalies hinder their broader use in real-world settings. To this end, in this paper, we propose a novel component-aware anomaly detection framework (ComAD) which can simultaneously achieve adjustable and logical anomaly detection for industrial scenarios. Specifically, we propose to segment images into multiple components based on a lightweight and nearly training-free unsupervised semantic segmentation model. Then, we design an interpretable logical anomaly detection model through modeling the metrological features of each component and their relationships. Despite its simplicity, our framework achieves state-of-the-art performance on image-level logical anomaly detection. Meanwhile, segmenting a product image into multiple components provides a novel perspective for industrial visual inspection, demonstrating great potential in model customization, noise resistance, and anomaly classification. The code will be available at https://github.com/liutongkun/ComAD.
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2304.02216.pdf' target='_blank'>https://arxiv.org/pdf/2304.02216.pdf</a></span>   <span><a href='https://github.com/zhangzilongc/MMR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilong Zhang, Zhibin Zhao, Xingwu Zhang, Chuang Sun, Xuefeng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.02216">Industrial Anomaly Detection with Domain Shift: A Real-world Dataset and Masked Multi-scale Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection (IAD) is crucial for automating industrial quality inspection. The diversity of the datasets is the foundation for developing comprehensive IAD algorithms. Existing IAD datasets focus on the diversity of data categories, overlooking the diversity of domains within the same data category. In this paper, to bridge this gap, we propose the Aero-engine Blade Anomaly Detection (AeBAD) dataset, consisting of two sub-datasets: the single-blade dataset and the video anomaly detection dataset of blades. Compared to existing datasets, AeBAD has the following two characteristics: 1.) The target samples are not aligned and at different scales. 2.) There is a domain shift between the distribution of normal samples in the test set and the training set, where the domain shifts are mainly caused by the changes in illumination and view. Based on this dataset, we observe that current state-of-the-art (SOTA) IAD methods exhibit limitations when the domain of normal samples in the test set undergoes a shift. To address this issue, we propose a novel method called masked multi-scale reconstruction (MMR), which enhances the model's capacity to deduce causality among patches in normal samples by a masked reconstruction task. MMR achieves superior performance compared to SOTA methods on the AeBAD dataset. Furthermore, MMR achieves competitive performance with SOTA methods to detect the anomalies of different types on the MVTec AD dataset. Code and dataset are available at https://github.com/zhangzilongc/MMR.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2303.16191.pdf' target='_blank'>https://arxiv.org/pdf/2303.16191.pdf</a></span>   <span><a href='https://github.com/NarcissusEx/HETMM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Chen, Xiaohua Xie, Lingxiao Yang, Jianhuang Lai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.16191">Hard-normal Example-aware Template Mutual Matching for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detectors are widely used in industrial manufacturing to detect and localize unknown defects in query images. These detectors are trained on anomaly-free samples and have successfully distinguished anomalies from most normal samples. However, hard-normal examples are scattered and far apart from most normal samples, and thus they are often mistaken for anomalies by existing methods. To address this issue, we propose Hard-normal Example-aware Template Mutual Matching (HETMM), an efficient framework to build a robust prototype-based decision boundary. Specifically, HETMM employs the proposed Affine-invariant Template Mutual Matching (ATMM) to mitigate the affection brought by the affine transformations and easy-normal examples. By mutually matching the pixel-level prototypes within the patch-level search spaces between query and template set, ATMM can accurately distinguish between hard-normal examples and anomalies, achieving low false-positive and missed-detection rates. In addition, we also propose PTS to compress the original template set for speed-up. PTS selects cluster centres and hard-normal examples to preserve the original decision boundary, allowing this tiny set to achieve comparable performance to the original one. Extensive experiments demonstrate that HETMM outperforms state-of-the-art methods, while using a 60-sheet tiny set can achieve competitive performance and real-time inference speed (around 26.1 FPS) on a Quadro 8000 RTX GPU. HETMM is training-free and can be hot-updated by directly inserting novel samples into the template set, which can promptly address some incremental learning issues in industrial manufacturing.
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2303.00601.pdf' target='_blank'>https://arxiv.org/pdf/2303.00601.pdf</a></span>   <span><a href='https://github.com/nomewang/M3DM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Wang, Jinlong Peng, Jiangning Zhang, Ran Yi, Yabiao Wang, Chengjie Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.00601">Multimodal Industrial Anomaly Detection via Hybrid Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>2D-based Industrial Anomaly Detection has been widely discussed, however, multimodal industrial anomaly detection based on 3D point clouds and RGB images still has many untouched fields. Existing multimodal industrial anomaly detection methods directly concatenate the multimodal features, which leads to a strong disturbance between features and harms the detection performance. In this paper, we propose Multi-3D-Memory (M3DM), a novel multimodal anomaly detection method with hybrid fusion scheme: firstly, we design an unsupervised feature fusion with patch-wise contrastive learning to encourage the interaction of different modal features; secondly, we use a decision layer fusion with multiple memory banks to avoid loss of information and additional novelty classifiers to make the final decision. We further propose a point feature alignment operation to better align the point cloud and RGB features. Extensive experiments show that our multimodal industrial anomaly detection model outperforms the state-of-the-art (SOTA) methods on both detection and segmentation precision on MVTec-3D AD dataset. Code is available at https://github.com/nomewang/M3DM.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2301.11514.pdf' target='_blank'>https://arxiv.org/pdf/2301.11514.pdf</a></span>   <span><a href='https://github.com/M-3LAB/awesome-industrial-anomaly-detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Liu, Guoyang Xie, Jinbao Wang, Shangnian Li, Chengjie Wang, Feng Zheng, Yaochu Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.11514">Deep Industrial Image Anomaly Detection: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent rapid development of deep learning has laid a milestone in industrial Image Anomaly Detection (IAD). In this paper, we provide a comprehensive review of deep learning-based image anomaly detection techniques, from the perspectives of neural network architectures, levels of supervision, loss functions, metrics and datasets. In addition, we extract the new setting from industrial manufacturing and review the current IAD approaches under our proposed our new setting. Moreover, we highlight several opening challenges for image anomaly detection. The merits and downsides of representative network architectures under varying supervision are discussed. Finally, we summarize the research findings and point out future research directions. More resources are available at https://github.com/M-3LAB/awesome-industrial-anomaly-detection.
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2308.04789.pdf' target='_blank'>https://arxiv.org/pdf/2308.04789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoqin Huang, Aofan Jiang, Ya Zhang, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04789">Multi-Scale Memory Comparison for Zero-/Few-Shot Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection has gained considerable attention due to its broad range of applications, particularly in industrial defect detection. To address the challenges of data collection, researchers have introduced zero-/few-shot anomaly detection techniques that require minimal normal images for each category. However, complex industrial scenarios often involve multiple objects, presenting a significant challenge. In light of this, we propose a straightforward yet powerful multi-scale memory comparison framework for zero-/few-shot anomaly detection. Our approach employs a global memory bank to capture features across the entire image, while an individual memory bank focuses on simplified scenes containing a single object. The efficacy of our method is validated by its remarkable achievement of 4th place in the zero-shot track and 2nd place in the few-shot track of the Visual Anomaly and Novelty Detection (VAND) competition.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2504.14221.pdf' target='_blank'>https://arxiv.org/pdf/2504.14221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbing Zhu, Lidong Wang, Ziqing Zhou, Chengjie Wang, Yurui Pan, Ruoyi Zhang, Zhuhao Chen, Linjie Cheng, Bin-Bin Gao, Jiangning Zhang, Zhenye Gan, Yuxie Wang, Yulong Chen, Shuguang Qian, Mingmin Chi, Bo Peng, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14221">Real-IAD D3: A Real-World 2D/Pseudo-3D/3D Dataset for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing complexity of industrial anomaly detection (IAD) has positioned multimodal detection methods as a focal area of machine vision research. However, dedicated multimodal datasets specifically tailored for IAD remain limited. Pioneering datasets like MVTec 3D have laid essential groundwork in multimodal IAD by incorporating RGB+3D data, but still face challenges in bridging the gap with real industrial environments due to limitations in scale and resolution. To address these challenges, we introduce Real-IAD D3, a high-precision multimodal dataset that uniquely incorporates an additional pseudo3D modality generated through photometric stereo, alongside high-resolution RGB images and micrometer-level 3D point clouds. Real-IAD D3 features finer defects, diverse anomalies, and greater scale across 20 categories, providing a challenging benchmark for multimodal IAD Additionally, we introduce an effective approach that integrates RGB, point cloud, and pseudo-3D depth information to leverage the complementary strengths of each modality, enhancing detection performance. Our experiments highlight the importance of these modalities in boosting detection robustness and overall IAD performance. The dataset and code are publicly accessible for research purposes at https://realiad4ad.github.io/Real-IAD D3
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2406.02263.pdf' target='_blank'>https://arxiv.org/pdf/2406.02263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengjie Wang, Haokun Zhu, Jinlong Peng, Yue Wang, Ran Yi, Yunsheng Wu, Lizhuang Ma, Jiangning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02263">M3DM-NR: RGB-3D Noisy-Resistant Industrial Anomaly Detection via Multimodal Denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing industrial anomaly detection methods primarily concentrate on unsupervised learning with pristine RGB images. Yet, both RGB and 3D data are crucial for anomaly detection, and the datasets are seldom completely clean in practical scenarios. To address above challenges, this paper initially delves into the RGB-3D multi-modal noisy anomaly detection, proposing a novel noise-resistant M3DM-NR framework to leveraging strong multi-modal discriminative capabilities of CLIP. M3DM-NR consists of three stages: Stage-I introduces the Suspected References Selection module to filter a few normal samples from the training dataset, using the multimodal features extracted by the Initial Feature Extraction, and a Suspected Anomaly Map Computation module to generate a suspected anomaly map to focus on abnormal regions as reference. Stage-II uses the suspected anomaly maps of the reference samples as reference, and inputs image, point cloud, and text information to achieve denoising of the training samples through intra-modal comparison and multi-scale aggregation operations. Finally, Stage-III proposes the Point Feature Alignment, Unsupervised Feature Fusion, Noise Discriminative Coreset Selection, and Decision Layer Fusion modules to learn the pattern of the training dataset, enabling anomaly detection and segmentation while filtering out noise. Extensive experiments show that M3DM-NR outperforms state-of-the-art methods in 3D-RGB multi-modal noisy anomaly detection.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2403.12362.pdf' target='_blank'>https://arxiv.org/pdf/2403.12362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianlong Hu, Xu Chen, Zhenye Gan, Jinlong Peng, Shengchuan Zhang, Jiangning Zhang, Yabiao Wang, Chengjie Wang, Liujuan Cao, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12362">DMAD: Dual Memory Bank for Real-World Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training a unified model is considered to be more suitable for practical industrial anomaly detection scenarios due to its generalization ability and storage efficiency. However, this multi-class setting, which exclusively uses normal data, overlooks the few but important accessible annotated anomalies in the real world. To address the challenge of real-world anomaly detection, we propose a new framework named Dual Memory bank enhanced representation learning for Anomaly Detection (DMAD). This framework handles both unsupervised and semi-supervised scenarios in a unified (multi-class) setting. DMAD employs a dual memory bank to calculate feature distance and feature attention between normal and abnormal patterns, thereby encapsulating knowledge about normal and abnormal instances. This knowledge is then used to construct an enhanced representation for anomaly score learning. We evaluated DMAD on the MVTec-AD and VisA datasets. The results show that DMAD surpasses current state-of-the-art methods, highlighting DMAD's capability in handling the complexities of real-world anomaly detection scenarios.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2401.03145.pdf' target='_blank'>https://arxiv.org/pdf/2401.03145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpeng Tu, Boshen Zhang, Liang Liu, Yuxi Li, Xuhai Chen, Jiangning Zhang, Yabiao Wang, Chengjie Wang, Cai Rong Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.03145">Self-supervised Feature Adaptation for 3D Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection is generally addressed as an unsupervised task that aims at locating defects with only normal training samples. Recently, numerous 2D anomaly detection methods have been proposed and have achieved promising results, however, using only the 2D RGB data as input is not sufficient to identify imperceptible geometric surface anomalies. Hence, in this work, we focus on multi-modal anomaly detection. Specifically, we investigate early multi-modal approaches that attempted to utilize models pre-trained on large-scale visual datasets, i.e., ImageNet, to construct feature databases. And we empirically find that directly using these pre-trained models is not optimal, it can either fail to detect subtle defects or mistake abnormal features as normal ones. This may be attributed to the domain gap between target industrial data and source data.Towards this problem, we propose a Local-to-global Self-supervised Feature Adaptation (LSFA) method to finetune the adaptors and learn task-oriented representation toward anomaly detection.Both intra-modal adaptation and cross-modal alignment are optimized from a local-to-global perspective in LSFA to ensure the representation quality and consistency in the inference stage.Extensive experiments demonstrate that our method not only brings a significant performance boost to feature embedding based approaches, but also outperforms previous State-of-The-Art (SoTA) methods prominently on both MVTec-3D AD and Eyecandies datasets, e.g., LSFA achieves 97.1% I-AUROC on MVTec-3D, surpass previous SoTA by +3.4%.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2407.03130.pdf' target='_blank'>https://arxiv.org/pdf/2407.03130.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxi Li, Jingqi Wu, Lin Yuanbo Wu, Hao Chen, Deyin Liu, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.03130">Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of practical Anomaly Detection (AD) tasks, manual labeling of anomalous pixels proves to be a costly endeavor. Consequently, many AD methods are crafted as one-class classifiers, tailored for training sets completely devoid of anomalies, ensuring a more cost-effective approach. While some pioneering work has demonstrated heightened AD accuracy by incorporating real anomaly samples in training, this enhancement comes at the price of labor-intensive labeling processes. This paper strikes the balance between AD accuracy and labeling expenses by introducing ADClick, a novel Interactive Image Segmentation (IIS) algorithm. ADClick efficiently generates "ground-truth" anomaly masks for real defective images, leveraging innovative residual features and meticulously crafted language prompts. Notably, ADClick showcases a significantly elevated generalization capacity compared to existing state-of-the-art IIS approaches. Functioning as an anomaly labeling tool, ADClick generates high-quality anomaly labels (AP $= 94.1\%$ on MVTec AD) based on only $3$ to $5$ manual click annotations per training image. Furthermore, we extend the capabilities of ADClick into ADClick-Seg, an enhanced model designed for anomaly detection and localization. By fine-tuning the ADClick-Seg model using the weak labels inferred by ADClick, we establish the state-of-the-art performances in supervised AD tasks (AP $= 86.4\%$ on MVTec AD and AP $= 78.4\%$, PRO $= 98.6\%$ on KSDD2).
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2306.03492.pdf' target='_blank'>https://arxiv.org/pdf/2306.03492.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxi Li, Jingqi Wu, Deyin Liu, Lin Wu, Hao Chen, Mingwen Wang, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03492">Industrial Anomaly Detection and Localization Using Weakly-Supervised Residual Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in industrial anomaly detection (AD) have demonstrated that incorporating a small number of anomalous samples during training can significantly enhance accuracy. However, this improvement often comes at the cost of extensive annotation efforts, which are impractical for many real-world applications. In this paper, we introduce a novel framework, Weak}ly-supervised RESidual Transformer (WeakREST), designed to achieve high anomaly detection accuracy while minimizing the reliance on manual annotations. First, we reformulate the pixel-wise anomaly localization task into a block-wise classification problem. Second, we introduce a residual-based feature representation called Positional Fast Anomaly Residuals (PosFAR) which captures anomalous patterns more effectively. To leverage this feature, we adapt the Swin Transformer for enhanced anomaly detection and localization. Additionally, we propose a weak annotation approach, utilizing bounding boxes and image tags to define anomalous regions. This approach establishes a semi-supervised learning context that reduces the dependency on precise pixel-level labels. To further improve the learning process, we develop a novel ResMixMatch algorithm, capable of handling the interplay between weak labels and residual-based representations.
  On the benchmark dataset MVTec-AD, our method achieves an Average Precision (AP) of $83.0\%$, surpassing the previous best result of $82.7\%$ in the unsupervised setting. In the supervised AD setting, WeakREST attains an AP of $87.6\%$, outperforming the previous best of $86.0\%$. Notably, even when using weaker annotations such as bounding boxes, WeakREST exceeds the performance of leading methods relying on pixel-wise supervision, achieving an AP of $87.1\%$ compared to the prior best of $86.0\%$ on MVTec-AD.
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2410.01737.pdf' target='_blank'>https://arxiv.org/pdf/2410.01737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingchen Miao, Wenqiao Zhang, Juncheng Li, Siliang Tang, Zhaocheng Li, Haochen Shi, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01737">RADAR: Robust Two-stage Modality-incomplete Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal Industrial Anomaly Detection (MIAD), utilizing 3D point clouds and 2D RGB images to identify the abnormal region of products, plays a crucial role in industrial quality inspection. However, the conventional MIAD setting presupposes that all 2D and 3D modalities are paired, overlooking the fact that multimodal data collected from the real world is often imperfect due to missing modalities. Consequently, MIAD models that demonstrate robustness against modal-incomplete data are highly desirable in practice. To address this practical challenge, we introduce a first-of-its-kind study that comprehensively investigates Modality-Incomplete Industrial Anomaly Detection (MIIAD), to consider the imperfect learning environment in which the multimodal information may be incomplete. Not surprisingly, we discovered that most existing MIAD approaches are inadequate for addressing MIIAD challenges, leading to significant performance degradation on the MIIAD benchmark we developed. In this paper, we propose a novel two-stage Robust modAlity-imcomplete fusing and Detecting frAmewoRk, abbreviated as RADAR. Our bootstrapping philosophy is to enhance two stages in MIIAD, improving the robustness of the Multimodal Transformer: i) In feature fusion, we first explore learning modality-incomplete instruction, guiding the pre-trained Multimodal Transformer to robustly adapt to various modality-incomplete scenarios, and implement adaptive parameter learning based on a HyperNetwork; ii) In anomaly detection, we construct a real-pseudo hybrid module to highlight the distinctiveness of modality combinations, further enhancing the robustness of the MIIAD model. Our experimental results demonstrate that the proposed RADAR significantly surpasses conventional MIAD methods in terms of effectiveness and robustness on our newly created MIIAD dataset, underscoring its practical application value.
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2310.02821.pdf' target='_blank'>https://arxiv.org/pdf/2310.02821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Chen, Kaihang Pan, Guoming Wang, Yueting Zhuang, Siliang Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02821">Improving Vision Anomaly Detection with the Guidance of Language Modality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have seen a surge of interest in anomaly detection for tackling industrial defect detection, event detection, etc. However, existing unsupervised anomaly detectors, particularly those for the vision modality, face significant challenges due to redundant information and sparse latent space. Conversely, the language modality performs well due to its relatively single data. This paper tackles the aforementioned challenges for vision modality from a multimodal point of view. Specifically, we propose Cross-modal Guidance (CMG), which consists of Cross-modal Entropy Reduction (CMER) and Cross-modal Linear Embedding (CMLE), to tackle the redundant information issue and sparse space issue, respectively. CMER masks parts of the raw image and computes the matching score with the text. Then, CMER discards irrelevant pixels to make the detector focus on critical contents. To learn a more compact latent space for the vision anomaly detector, CMLE learns a correlation structure matrix from the language modality, and then the latent space of vision modality will be learned with the guidance of the matrix. Thereafter, the vision latent space will get semantically similar images closer. Extensive experiments demonstrate the effectiveness of the proposed methods. Particularly, CMG outperforms the baseline that only uses images by 16.81%. Ablation experiments further confirm the synergy among the proposed methods, as each component depends on the other to achieve optimal performance.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2312.03804.pdf' target='_blank'>https://arxiv.org/pdf/2312.03804.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Felix Meissen, Johannes Getzner, Alexander Ziller, ÃzgÃ¼n Turgut, Georgios Kaissis, Martin J. Menten, Daniel Rueckert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03804">How Low Can You Go? Surfacing Prototypical In-Distribution Samples for Unsupervised Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised anomaly detection (UAD) alleviates large labeling efforts by training exclusively on unlabeled in-distribution data and detecting outliers as anomalies. Generally, the assumption prevails that large training datasets allow the training of higher-performing UAD models. However, in this work, we show that UAD with extremely few training samples can already match -- and in some cases even surpass -- the performance of training with the whole training dataset. Building upon this finding, we propose an unsupervised method to reliably identify prototypical samples to further boost UAD performance. We demonstrate the utility of our method on seven different established UAD benchmarks from computer vision, industrial defect detection, and medicine. With just 25 selected samples, we even exceed the performance of full training in $25/67$ categories in these benchmarks. Additionally, we show that the prototypical in-distribution samples identified by our proposed method generalize well across models and datasets and that observing their sample selection criteria allows for a successful manual selection of small subsets of high-performing samples. Our code is available at https://anonymous.4open.science/r/uad_prototypical_samples/
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2504.12970.pdf' target='_blank'>https://arxiv.org/pdf/2504.12970.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Qian, Bingke Zhu, Yingying Chen, Ming Tang, Jinqiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12970">MathPhys-Guided Coarse-to-Fine Anomaly Synthesis with SQE-Driven Bi-Level Optimization for Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Currently, industrial anomaly detection suffers from two bottlenecks: (i) the rarity of real-world defect images and (ii) the opacity of sample quality when synthetic data are used. Existing synthetic strategies (e.g., cut-and-paste) overlook the underlying physical causes of defects, leading to inconsistent, low-fidelity anomalies that hamper model generalization to real-world complexities. In this paper, we introduce a novel and lightweight pipeline that generates synthetic anomalies through Math-Phys model guidance, refines them via a Coarse-to-Fine approach and employs a bi-level optimization strategy with a Synthesis Quality Estimator (SQE). By combining physical modeling of the three most typical physics-driven defect mechanisms: Fracture Line (FL), Pitting Loss (PL), and Plastic Warpage (PW), our method produces realistic defect masks, which are subsequently enhanced in two phases. The first stage (npcF) enforces a PDE-based consistency to achieve a globally coherent anomaly structure, while the second stage (npcF++) further improves local fidelity. Additionally, we leverage SQE-driven weighting, ensuring that high-quality synthetic samples receive greater emphasis during training. To validate our method, we conduct experiments on three anomaly detection benchmarks: MVTec AD, VisA, and BTAD. Across these datasets, our method achieves state-of-the-art results in both image- and pixel-AUROC, confirming the effectiveness of our MaPhC2F dataset and BiSQAD method. All code will be released.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2409.20146.pdf' target='_blank'>https://arxiv.org/pdf/2409.20146.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huilin Deng, Hongchen Luo, Wei Zhai, Yang Cao, Yu Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.20146">VMAD: Visual-enhanced Multimodal Large Language Model for Zero-Shot Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Zero-shot anomaly detection (ZSAD) recognizes and localizes anomalies in previously unseen objects by establishing feature mapping between textual prompts and inspection images, demonstrating excellent research value in flexible industrial manufacturing. However, existing ZSAD methods are limited by closed-world settings, struggling to unseen defects with predefined prompts. Recently, adapting Multimodal Large Language Models (MLLMs) for Industrial Anomaly Detection (IAD) presents a viable solution. Unlike fixed-prompt methods, MLLMs exhibit a generative paradigm with open-ended text interpretation, enabling more adaptive anomaly analysis. However, this adaption faces inherent challenges as anomalies often manifest in fine-grained regions and exhibit minimal visual discrepancies from normal samples. To address these challenges, we propose a novel framework VMAD (Visual-enhanced MLLM Anomaly Detection) that enhances MLLM with visual-based IAD knowledge and fine-grained perception, simultaneously providing precise detection and comprehensive analysis of anomalies. Specifically, we design a Defect-Sensitive Structure Learning scheme that transfers patch-similarities cues from visual branch to our MLLM for improved anomaly discrimination. Besides, we introduce a novel visual projector, Locality-enhanced Token Compression, which mines multi-level features in local contexts to enhance fine-grained detection. Furthermore, we introduce the Real Industrial Anomaly Detection (RIAD), a comprehensive IAD dataset with detailed anomaly descriptions and analyses, offering a valuable resource for MLLM-based IAD development. Extensive experiments on zero-shot benchmarks, including MVTec-AD, Visa, WFDD, and RIAD datasets, demonstrate our superior performance over state-of-the-art methods. The code and dataset will be available soon.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2404.05231.pdf' target='_blank'>https://arxiv.org/pdf/2404.05231.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofan Li, Zhizhong Zhang, Xin Tan, Chengwei Chen, Yanyun Qu, Yuan Xie, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05231">PromptAD: Learning Prompts with only Normal Samples for Few-Shot Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vision-language model has brought great improvement to few-shot industrial anomaly detection, which usually needs to design of hundreds of prompts through prompt engineering. For automated scenarios, we first use conventional prompt learning with many-class paradigm as the baseline to automatically learn prompts but found that it can not work well in one-class anomaly detection. To address the above problem, this paper proposes a one-class prompt learning method for few-shot anomaly detection, termed PromptAD. First, we propose semantic concatenation which can transpose normal prompts into anomaly prompts by concatenating normal prompts with anomaly suffixes, thus constructing a large number of negative samples used to guide prompt learning in one-class setting. Furthermore, to mitigate the training challenge caused by the absence of anomaly images, we introduce the concept of explicit anomaly margin, which is used to explicitly control the margin between normal prompt features and anomaly prompt features through a hyper-parameter. For image-level/pixel-level anomaly detection, PromptAD achieves first place in 11/12 few-shot settings on MVTec and VisA.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2412.08189.pdf' target='_blank'>https://arxiv.org/pdf/2412.08189.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Chen, Liujuan Cao, Shengchuan Zhang, Xiewu Zheng, Yan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08189">Breaking the Bias: Recalibrating the Attention of Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the scarcity and unpredictable nature of defect samples, industrial anomaly detection (IAD) predominantly employs unsupervised learning. However, all unsupervised IAD methods face a common challenge: the inherent bias in normal samples, which causes models to focus on variable regions while overlooking potential defects in invariant areas. To effectively overcome this, it is essential to decompose and recalibrate attention, guiding the model to suppress irrelevant variations and concentrate on subtle, defect-susceptible areas. In this paper, we propose Recalibrating Attention of Industrial Anomaly Detection (RAAD), a framework that systematically decomposes and recalibrates attention maps. RAAD employs a two-stage process: first, it reduces attention bias through quantization, and second, it fine-tunes defect-prone regions for improved sensitivity. Central to this framework is Hierarchical Quantization Scoring (HQS), which dynamically allocates bit-widths across layers based on their anomaly detection contributions. HQS dynamically adjusts bit-widths based on the hierarchical nature of attention maps, compressing lower layers that produce coarse and noisy attention while preserving deeper layers with sharper, defect-focused attention. This approach optimizes both computational efficiency and the model' s sensitivity to anomalies. We validate the effectiveness of RAAD on 32 datasets using a single 3090ti. Experiments demonstrate that RAAD, balances the complexity and expressive power of the model, enhancing its anomaly detection capability.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2302.09794.pdf' target='_blank'>https://arxiv.org/pdf/2302.09794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaewon Park, Minhyeok Lee, Suhwan Cho, Donghyeong Kim, Sangyoun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.09794">Two-stream Decoder Feature Normality Estimating Network for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image reconstruction-based anomaly detection has recently been in the spotlight because of the difficulty of constructing anomaly datasets. These approaches work by learning to model normal features without seeing abnormal samples during training and then discriminating anomalies at test time based on the reconstructive errors. However, these models have limitations in reconstructing the abnormal samples due to their indiscriminate conveyance of features. Moreover, these approaches are not explicitly optimized for distinguishable anomalies. To address these problems, we propose a two-stream decoder network (TSDN), designed to learn both normal and abnormal features. Additionally, we propose a feature normality estimator (FNE) to eliminate abnormal features and prevent high-quality reconstruction of abnormal regions. Evaluation on a standard benchmark demonstrated performance better than state-of-the-art models.
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2211.07381.pdf' target='_blank'>https://arxiv.org/pdf/2211.07381.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Donghyeong Kim, Chaewon Park, Suhwan Cho, Sangyoun Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.07381">FAPM: Fast Adaptive Patch Memory for Real-time Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feature embedding-based methods have shown exceptional performance in detecting industrial anomalies by comparing features of target images with normal images. However, some methods do not meet the speed requirements of real-time inference, which is crucial for real-world applications. To address this issue, we propose a new method called Fast Adaptive Patch Memory (FAPM) for real-time industrial anomaly detection. FAPM utilizes patch-wise and layer-wise memory banks that store the embedding features of images at the patch and layer level, respectively, which eliminates unnecessary repetitive computations. We also propose patch-wise adaptive coreset sampling for faster and more accurate detection. FAPM performs well in both accuracy and speed compared to other state-of-the-art methods
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2507.21619.pdf' target='_blank'>https://arxiv.org/pdf/2507.21619.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Guan, Jun Lan, Jian Cao, Hao Tan, Huijia Zhu, Weiqiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21619">EMIT: Enhancing MLLMs for Industrial Anomaly Detection via Difficulty-Aware GRPO</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection (IAD) plays a crucial role in maintaining the safety and reliability of manufacturing systems. While multimodal large language models (MLLMs) show strong vision-language reasoning abilities, their effectiveness in IAD remains limited without domain-specific adaptation. In this work, we propose EMIT, a unified framework that enhances MLLMs for IAD via difficulty-aware group relative policy optimization (GRPO). EMIT constructs a multi-task IAD dataset and utilizes GPT-generated object text descriptions to compensate for missing defective images. For few-shot anomaly detection, it integrates a soft prompt and heatmap-guided contrastive embeddings derived from patch-level comparisons. To better handle difficult data samples, i.e., cases where the MLLM struggles to generate correct answers, we propose a difficulty-aware GRPO that extends the original GRPO by incorporating a response resampling strategy to ensure the inclusion of correct answers in the sampled responses, as well as an advantage reweighting mechanism to strengthen learning from such difficult data samples. Extensive experiments on the MMAD benchmark demonstrate that EMIT significantly enhances the IAD performance of MLLMs, achieving an average improvement of 7.77\% over the base model (InternVL3-8B) across seven tasks.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2507.13378.pdf' target='_blank'>https://arxiv.org/pdf/2507.13378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Cheng, Yunkang Cao, Haiming Yao, Wei Luo, Cheng Jiang, Hui Zhang, Weiming Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13378">A Comprehensive Survey for Real-World Industrial Defect Detection: Challenges, Approaches, and Prospects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial defect detection is vital for upholding product quality across contemporary manufacturing systems. As the expectations for precision, automation, and scalability intensify, conventional inspection approaches are increasingly found wanting in addressing real-world demands. Notable progress in computer vision and deep learning has substantially bolstered defect detection capabilities across both 2D and 3D modalities. A significant development has been the pivot from closed-set to open-set defect detection frameworks, which diminishes the necessity for extensive defect annotations and facilitates the recognition of novel anomalies. Despite such strides, a cohesive and contemporary understanding of industrial defect detection remains elusive. Consequently, this survey delivers an in-depth analysis of both closed-set and open-set defect detection strategies within 2D and 3D modalities, charting their evolution in recent years and underscoring the rising prominence of open-set techniques. We distill critical challenges inherent in practical detection environments and illuminate emerging trends, thereby providing a current and comprehensive vista of this swiftly progressing field.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2406.11507.pdf' target='_blank'>https://arxiv.org/pdf/2406.11507.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiming Yao, Yunkang Cao, Wei Luo, Weihang Zhang, Wenyong Yu, Weiming Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11507">Prior Normality Prompt Transformer for Multi-class Industrial Image Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image anomaly detection plays a pivotal role in industrial inspection. Traditional approaches often demand distinct models for specific categories, resulting in substantial deployment costs. This raises concerns about multi-class anomaly detection, where a unified model is developed for multiple classes. However, applying conventional methods, particularly reconstruction-based models, directly to multi-class scenarios encounters challenges such as identical shortcut learning, hindering effective discrimination between normal and abnormal instances. To tackle this issue, our study introduces the Prior Normality Prompt Transformer (PNPT) method for multi-class image anomaly detection. PNPT strategically incorporates normal semantics prompting to mitigate the "identical mapping" problem. This entails integrating a prior normality prompt into the reconstruction process, yielding a dual-stream model. This innovative architecture combines normal prior semantics with abnormal samples, enabling dual-stream reconstruction grounded in both prior knowledge and intrinsic sample characteristics. PNPT comprises four essential modules: Class-Specific Normality Prompting Pool (CS-NPP), Hierarchical Patch Embedding (HPE), Semantic Alignment Coupling Encoding (SACE), and Contextual Semantic Conditional Decoding (CSCD). Experimental validation on diverse benchmark datasets and real-world industrial applications highlights PNPT's superior performance in multi-class industrial anomaly detection.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2406.04687.pdf' target='_blank'>https://arxiv.org/pdf/2406.04687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Zhang, Yunkang Cao, Xiaohao Xu, Weiming Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04687">LogiCode: an LLM-Driven Framework for Logical Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents LogiCode, a novel framework that leverages Large Language Models (LLMs) for identifying logical anomalies in industrial settings, moving beyond traditional focus on structural inconsistencies. By harnessing LLMs for logical reasoning, LogiCode autonomously generates Python codes to pinpoint anomalies such as incorrect component quantities or missing elements, marking a significant leap forward in anomaly detection technologies. A custom dataset "LOCO-Annotations" and a benchmark "LogiBench" are introduced to evaluate the LogiCode's performance across various metrics including binary classification accuracy, code generation success rate, and precision in reasoning. Findings demonstrate LogiCode's enhanced interpretability, significantly improving the accuracy of logical anomaly detection and offering detailed explanations for identified anomalies. This represents a notable shift towards more intelligent, LLM-driven approaches in industrial anomaly detection, promising substantial impacts on industry-specific applications.
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2405.02068.pdf' target='_blank'>https://arxiv.org/pdf/2405.02068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Canhui Tang, Sanping Zhou, Yizhe Li, Yonghao Dong, Le Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02068">Advancing Pre-trained Teacher: Towards Robust Feature Discrepancy for Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the wide application of knowledge distillation between an ImageNet pre-trained teacher model and a learnable student model, industrial anomaly detection has witnessed a significant achievement in the past few years. The success of knowledge distillation mainly relies on how to keep the feature discrepancy between the teacher and student model, in which it assumes that: (1) the teacher model can jointly represent two different distributions for the normal and abnormal patterns, while (2) the student model can only reconstruct the normal distribution. However, it still remains a challenging issue to maintain these ideal assumptions in practice. In this paper, we propose a simple yet effective two-stage industrial anomaly detection framework, termed as AAND, which sequentially performs Anomaly Amplification and Normality Distillation to obtain robust feature discrepancy. In the first anomaly amplification stage, we propose a novel Residual Anomaly Amplification (RAA) module to advance the pre-trained teacher encoder. With the exposure of synthetic anomalies, it amplifies anomalies via residual generation while maintaining the integrity of pre-trained model. It mainly comprises a Matching-guided Residual Gate and an Attribute-scaling Residual Generator, which can determine the residuals' proportion and characteristic, respectively. In the second normality distillation stage, we further employ a reverse distillation paradigm to train a student decoder, in which a novel Hard Knowledge Distillation (HKD) loss is built to better facilitate the reconstruction of normal patterns. Comprehensive experiments on the MvTecAD, VisA, and MvTec3D-RGB datasets show that our method achieves state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2406.00625.pdf' target='_blank'>https://arxiv.org/pdf/2406.00625.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Peng, Xiao Lin, Nachuan Ma, Jiayuan Du, Chuangwei Liu, Chengju Liu, Qijun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00625">SAM-LAD: Segment Anything Model Meets Zero-Shot Logic Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual anomaly detection is vital in real-world applications, such as industrial defect detection and medical diagnosis. However, most existing methods focus on local structural anomalies and fail to detect higher-level functional anomalies under logical conditions. Although recent studies have explored logical anomaly detection, they can only address simple anomalies like missing or addition and show poor generalizability due to being heavily data-driven. To fill this gap, we propose SAM-LAD, a zero-shot, plug-and-play framework for logical anomaly detection in any scene. First, we obtain a query image's feature map using a pre-trained backbone. Simultaneously, we retrieve the reference images and their corresponding feature maps via the nearest neighbor search of the query image. Then, we introduce the Segment Anything Model (SAM) to obtain object masks of the query and reference images. Each object mask is multiplied with the entire image's feature map to obtain object feature maps. Next, an Object Matching Model (OMM) is proposed to match objects in the query and reference images. To facilitate object matching, we further propose a Dynamic Channel Graph Attention (DCGA) module, treating each object as a keypoint and converting its feature maps into feature vectors. Finally, based on the object matching relations, an Anomaly Measurement Model (AMM) is proposed to detect objects with logical anomalies. Structural anomalies in the objects can also be detected. We validate our proposed SAM-LAD using various benchmarks, including industrial datasets (MVTec Loco AD, MVTec AD), and the logical dataset (DigitAnatomy). Extensive experimental results demonstrate that SAM-LAD outperforms existing SoTA methods, particularly in detecting logical anomalies.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2503.14162.pdf' target='_blank'>https://arxiv.org/pdf/2503.14162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongyun Zhang, Jiacheng Ruan, Xian Gao, Ting Liu, Yuzhuo Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14162">EIAD: Explainable Industrial Anomaly Detection Via Multi-Modal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial Anomaly Detection (IAD) is critical to ensure product quality during manufacturing. Although existing zero-shot defect segmentation and detection methods have shown effectiveness, they cannot provide detailed descriptions of the defects. Furthermore, the application of large multi-modal models in IAD remains in its infancy, facing challenges in balancing question-answering (QA) performance and mask-based grounding capabilities, often owing to overfitting during the fine-tuning process. To address these challenges, we propose a novel approach that introduces a dedicated multi-modal defect localization module to decouple the dialog functionality from the core feature extraction. This decoupling is achieved through independent optimization objectives and tailored learning strategies. Additionally, we contribute to the first multi-modal industrial anomaly detection training dataset, named Defect Detection Question Answering (DDQA), encompassing a wide range of defect types and industrial scenarios. Unlike conventional datasets that rely on GPT-generated data, DDQA ensures authenticity and reliability and offers a robust foundation for model training. Experimental results demonstrate that our proposed method, Explainable Industrial Anomaly Detection Assistant (EIAD), achieves outstanding performance in defect detection and localization tasks. It not only significantly enhances accuracy but also improves interpretability. These advancements highlight the potential of EIAD for practical applications in industrial settings.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2503.03562.pdf' target='_blank'>https://arxiv.org/pdf/2503.03562.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.03562">Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Humans detect real-world object anomalies by perceiving, interacting, and reasoning based on object-conditioned physical knowledge. The long-term goal of Industrial Anomaly Detection (IAD) is to enable machines to autonomously replicate this skill. However, current IAD algorithms are largely developed and tested on static, semantically simple datasets, which diverge from real-world scenarios where physical understanding and reasoning are essential. To bridge this gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, the first large-scale, real-world, physics-grounded video dataset for industrial anomaly detection. Collected using a real robot arm and motor, Phys-AD provides a diverse set of dynamic, semantically rich scenarios. The dataset includes more than 6400 videos across 22 real-world object categories, interacting with robot arms and motors, and exhibits 47 types of anomalies. Anomaly detection in Phys-AD requires visual reasoning, combining both physical knowledge and video content to determine object abnormality. We benchmark state-of-the-art anomaly detection methods under three settings: unsupervised AD, weakly-supervised AD, and video-understanding AD, highlighting their limitations in handling physics-grounded anomalies. Additionally, we introduce the Physics Anomaly Explanation (PAEval) metric, designed to assess the ability of visual-language foundation models to not only detect anomalies but also provide accurate explanations for their underlying physical causes. Our project is available at https://guyao2023.github.io/Phys-AD/.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2412.14592.pdf' target='_blank'>https://arxiv.org/pdf/2412.14592.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqiao Li, Bozhong Zheng, Xiaohao Xu, Jinye Gan, Fading Lu, Xiang Li, Na Ni, Zheng Tian, Xiaonan Huang, Shenghua Gao, Yingna Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.14592">Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry, and Internal Properties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Object anomaly detection is essential for industrial quality inspection, yet traditional single-sensor methods face critical limitations. They fail to capture the wide range of anomaly types, as single sensors are often constrained to either external appearance, geometric structure, or internal properties. To overcome these challenges, we introduce MulSen-AD, the first high-resolution, multi-sensor anomaly detection dataset tailored for industrial applications. MulSen-AD unifies data from RGB cameras, laser scanners, and lock-in infrared thermography, effectively capturing external appearance, geometric deformations, and internal defects. The dataset spans 15 industrial products with diverse, real-world anomalies. We also present MulSen-AD Bench, a benchmark designed to evaluate multi-sensor methods, and propose MulSen-TripleAD, a decision-level fusion algorithm that integrates these three modalities for robust, unsupervised object anomaly detection. Our experiments demonstrate that multi-sensor fusion substantially outperforms single-sensor approaches, achieving 96.1% AUROC in object-level detection accuracy. These results highlight the importance of integrating multi-sensor data for comprehensive industrial anomaly detection.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2407.06698.pdf' target='_blank'>https://arxiv.org/pdf/2407.06698.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengjie Wang, Chengming Xu, Zhenye Gan, Jianlong Hu, Wenbing Zhu, Lizhuag Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06698">PSPU: Enhanced Positive and Unlabeled Learning by Leveraging Pseudo Supervision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Positive and Unlabeled (PU) learning, a binary classification model trained with only positive and unlabeled data, generally suffers from overfitted risk estimation due to inconsistent data distributions. To address this, we introduce a pseudo-supervised PU learning framework (PSPU), in which we train the PU model first, use it to gather confident samples for the pseudo supervision, and then apply these supervision to correct the PU model's weights by leveraging non-PU objectives. We also incorporate an additional consistency loss to mitigate noisy sample effects. Our PSPU outperforms recent PU learning methods significantly on MNIST, CIFAR-10, CIFAR-100 in both balanced and imbalanced settings, and enjoys competitive performance on MVTecAD for industrial anomaly detection.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2403.12580.pdf' target='_blank'>https://arxiv.org/pdf/2403.12580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengjie Wang, Wenbing Zhu, Bin-Bin Gao, Zhenye Gan, Jianning Zhang, Zhihao Gu, Shuguang Qian, Mingang Chen, Lizhuang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12580">Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection (IAD) has garnered significant attention and experienced rapid development. However, the recent development of IAD approach has encountered certain difficulties due to dataset limitations. On the one hand, most of the state-of-the-art methods have achieved saturation (over 99% in AUROC) on mainstream datasets such as MVTec, and the differences of methods cannot be well distinguished, leading to a significant gap between public datasets and actual application scenarios. On the other hand, the research on various new practical anomaly detection settings is limited by the scale of the dataset, posing a risk of overfitting in evaluation results. Therefore, we propose a large-scale, Real-world, and multi-view Industrial Anomaly Detection dataset, named Real-IAD, which contains 150K high-resolution images of 30 different objects, an order of magnitude larger than existing datasets. It has a larger range of defect area and ratio proportions, making it more challenging than previous datasets. To make the dataset closer to real application scenarios, we adopted a multi-view shooting method and proposed sample-level evaluation metrics. In addition, beyond the general unsupervised anomaly detection setting, we propose a new setting for Fully Unsupervised Industrial Anomaly Detection (FUIAD) based on the observation that the yield rate in industrial production is usually greater than 60%, which has more practical application value. Finally, we report the results of popular IAD methods on the Real-IAD dataset, providing a highly challenging benchmark to promote the development of the IAD field.
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2204.11161.pdf' target='_blank'>https://arxiv.org/pdf/2204.11161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yajie Cui, Zhaoxiang Liu, Shiguo Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2204.11161">A Survey on Unsupervised Anomaly Detection Algorithms for Industrial Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In line with the development of Industry 4.0, surface defect detection/anomaly detection becomes a topical subject in the industry field. Improving efficiency as well as saving labor costs has steadily become a matter of great concern in practice, where deep learning-based algorithms perform better than traditional vision inspection methods in recent years. While existing deep learning-based algorithms are biased towards supervised learning, which not only necessitates a huge amount of labeled data and human labor, but also brings about inefficiency and limitations. In contrast, recent research shows that unsupervised learning has great potential in tackling the above disadvantages for visual industrial anomaly detection. In this survey, we summarize current challenges and provide a thorough overview of recently proposed unsupervised algorithms for visual industrial anomaly detection covering five categories, whose innovation points and frameworks are described in detail. Meanwhile, publicly available datasets for industrial anomaly detection are introduced. By comparing different classes of methods, the advantages and disadvantages of anomaly detection algorithms are summarized. Based on the current research framework, we point out the core issue that remains to be resolved and provide further improvement directions. Meanwhile, based on the latest technological trends, we offer insights into future research directions. It is expected to assist both the research community and industry in developing a broader and cross-domain perspective.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2508.10681.pdf' target='_blank'>https://arxiv.org/pdf/2508.10681.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mengyang Zhao, Teng Fu, Haiyang Yu, Ke Niu, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10681">IADGPT: Unified LVLM for Few-Shot Industrial Anomaly Detection, Localization, and Reasoning via In-Context Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-Shot Industrial Anomaly Detection (FS-IAD) has important applications in automating industrial quality inspection. Recently, some FS-IAD methods based on Large Vision-Language Models (LVLMs) have been proposed with some achievements through prompt learning or fine-tuning. However, existing LVLMs focus on general tasks but lack basic industrial knowledge and reasoning capabilities related to FS-IAD, making these methods far from specialized human quality inspectors. To address these challenges, we propose a unified framework, IADGPT, designed to perform FS-IAD in a human-like manner, while also handling associated localization and reasoning tasks, even for diverse and novel industrial products. To this end, we introduce a three-stage progressive training strategy inspired by humans. Specifically, the first two stages gradually guide IADGPT in acquiring fundamental industrial knowledge and discrepancy awareness. In the third stage, we design an in-context learning-based training paradigm, enabling IADGPT to leverage a few-shot image as the exemplars for improved generalization to novel products. In addition, we design a strategy that enables IADGPT to output image-level and pixel-level anomaly scores using the logits output and the attention map, respectively, in conjunction with the language output to accomplish anomaly reasoning. To support our training, we present a new dataset comprising 100K images across 400 diverse industrial product categories with extensive attribute-level textual annotations. Experiments indicate IADGPT achieves considerable performance gains in anomaly detection and demonstrates competitiveness in anomaly localization and reasoning. We will release our dataset in camera-ready.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2501.15795.pdf' target='_blank'>https://arxiv.org/pdf/2501.15795.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiling Chen, Hanning Chen, Mohsen Imani, Farhad Imani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15795">Can Multimodal Large Language Models be Guided to Improve Industrial Anomaly Detection?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In industrial settings, the accurate detection of anomalies is essential for maintaining product quality and ensuring operational safety. Traditional industrial anomaly detection (IAD) models often struggle with flexibility and adaptability, especially in dynamic production environments where new defect types and operational changes frequently arise. Recent advancements in Multimodal Large Language Models (MLLMs) hold promise for overcoming these limitations by combining visual and textual information processing capabilities. MLLMs excel in general visual understanding due to their training on large, diverse datasets, but they lack domain-specific knowledge, such as industry-specific defect tolerance levels, which limits their effectiveness in IAD tasks. To address these challenges, we propose Echo, a novel multi-expert framework designed to enhance MLLM performance for IAD. Echo integrates four expert modules: Reference Extractor which provides a contextual baseline by retrieving similar normal images, Knowledge Guide which supplies domain-specific insights, Reasoning Expert which enables structured, stepwise reasoning for complex queries, and Decision Maker which synthesizes information from all modules to deliver precise, context-aware responses. Evaluated on the MMAD benchmark, Echo demonstrates significant improvements in adaptability, precision, and robustness, moving closer to meeting the demands of real-world industrial anomaly detection.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2502.11712.pdf' target='_blank'>https://arxiv.org/pdf/2502.11712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Tong, Yang Chang, Qing Zhao, Jiawen Yu, Boyang Wang, Junxiong Lin, Yuxuan Lin, Xinji Mai, Haoran Wang, Zeng Tao, Yan Wang, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11712">Component-aware Unsupervised Logical Anomaly Generation for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection is critical in industrial manufacturing for ensuring product quality and improving efficiency in automated processes. The scarcity of anomalous samples limits traditional detection methods, making anomaly generation essential for expanding the data repository. However, recent generative models often produce unrealistic anomalies increasing false positives, or require real-world anomaly samples for training. In this work, we treat anomaly generation as a compositional problem and propose ComGEN, a component-aware and unsupervised framework that addresses the gap in logical anomaly generation. Our method comprises a multi-component learning strategy to disentangle visual components, followed by subsequent generation editing procedures. Disentangled text-to-component pairs, revealing intrinsic logical constraints, conduct attention-guided residual mapping and model training with iteratively matched references across multiple scales. Experiments on the MVTecLOCO dataset confirm the efficacy of ComGEN, achieving the best AUROC score of 91.2%. Additional experiments on the real-world scenario of Diesel Engine and widely-used MVTecAD dataset demonstrate significant performance improvements when integrating simulated anomalies generated by ComGEN into automated production workflows.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2407.04092.pdf' target='_blank'>https://arxiv.org/pdf/2407.04092.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Costanzino, Pierluigi Zama Ramirez, Giuseppe Lisanti, Luigi Di Stefano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04092">Learning to Be a Transformer to Pinpoint Anomalies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To efficiently deploy strong, often pre-trained feature extractors, recent Industrial Anomaly Detection and Segmentation (IADS) methods process low-resolution images, e.g., 224x224 pixels, obtained by downsampling the original input images. However, while numerous industrial applications demand the identification of both large and small defects, downsampling the input image to a low resolution may hinder a method's ability to pinpoint tiny anomalies. We propose a novel Teacher--Student paradigm to leverage strong pre-trained features while processing high-resolution input images very efficiently. The core idea concerns training two shallow MLPs (the Students) by nominal images so as to mimic the mappings between the patch embeddings induced by the self-attention layers of a frozen vision Transformer (the Teacher). Indeed, learning these mappings sets forth a challenging pretext task that small-capacity models are unlikely to accomplish on out-of-distribution data such as anomalous images. Our method can spot anomalies from high-resolution images and runs way faster than competitors, achieving state-of-the-art performance on MVTec AD and the best segmentation results on VisA. We also propose novel evaluation metrics to capture robustness to defect size, i.e., the ability to preserve good localisation from large anomalies to tiny ones. Evaluating our method also by these metrics reveals its neatly superior performance.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2312.04521.pdf' target='_blank'>https://arxiv.org/pdf/2312.04521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Costanzino, Pierluigi Zama Ramirez, Giuseppe Lisanti, Luigi Di Stefano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04521">Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The paper explores the industrial multimodal Anomaly Detection (AD) task, which exploits point clouds and RGB images to localize anomalies. We introduce a novel light and fast framework that learns to map features from one modality to the other on nominal samples. At test time, anomalies are detected by pinpointing inconsistencies between observed and mapped features. Extensive experiments show that our approach achieves state-of-the-art detection and segmentation performance in both the standard and few-shot settings on the MVTec 3D-AD dataset while achieving faster inference and occupying less memory than previous multimodal AD methods. Moreover, we propose a layer-pruning technique to improve memory and time efficiency with a marginal sacrifice in performance.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2504.11914.pdf' target='_blank'>https://arxiv.org/pdf/2504.11914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Chao, Jie Liu, Jie Tang, Gangshan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11914">AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial Anomaly Detection (IAD) poses a formidable challenge due to the scarcity of defective samples, making it imperative to deploy models capable of robust generalization to detect unseen anomalies effectively. Traditional approaches, often constrained by hand-crafted features or domain-specific expert models, struggle to address this limitation, underscoring the need for a paradigm shift. We introduce AnomalyR1, a pioneering framework that leverages VLM-R1, a Multimodal Large Language Model (MLLM) renowned for its exceptional generalization and interpretability, to revolutionize IAD. By integrating MLLM with Group Relative Policy Optimization (GRPO), enhanced by our novel Reasoned Outcome Alignment Metric (ROAM), AnomalyR1 achieves a fully end-to-end solution that autonomously processes inputs of image and domain knowledge, reasons through analysis, and generates precise anomaly localizations and masks. Based on the latest multimodal IAD benchmark, our compact 3-billion-parameter model outperforms existing methods, establishing state-of-the-art results. As MLLM capabilities continue to advance, this study is the first to deliver an end-to-end VLM-based IAD solution that demonstrates the transformative potential of ROAM-enhanced GRPO, positioning our framework as a forward-looking cornerstone for next-generation intelligent anomaly detection systems in industrial applications with limited defective data.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2403.04306.pdf' target='_blank'>https://arxiv.org/pdf/2403.04306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Jiang, Xinyu Yan, Ge-Peng Ji, Keren Fu, Meijun Sun, Huan Xiong, Deng-Ping Fan, Fahad Shahbaz Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04306">Effectiveness Assessment of Recent Large Vision-Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of large vision-language models (LVLMs) represents a remarkable advance in the quest for artificial general intelligence. However, the model's effectiveness in both specialized and general tasks warrants further investigation. This paper endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive understanding of these novel models. To gauge their effectiveness in specialized tasks, we employ six challenging tasks in three different application scenarios: natural, healthcare, and industrial. These six tasks include salient/camouflaged/transparent object detection, as well as polyp detection, skin lesion detection, and industrial anomaly detection. We examine the performance of three recent open-source LVLMs, including MiniGPT-v2, LLaVA-1.5, and Shikra, on both visual recognition and localization in these tasks. Moreover, we conduct empirical investigations utilizing the aforementioned LVLMs together with GPT-4V, assessing their multi-modal understanding capabilities in general tasks including object counting, absurd question answering, affordance reasoning, attribute recognition, and spatial relation reasoning. Our investigations reveal that these LVLMs demonstrate limited proficiency not only in specialized tasks but also in general tasks. We delve deep into this inadequacy and uncover several potential factors, including limited cognition in specialized tasks, object hallucination, text-to-image interference, and decreased robustness in complex problems. We hope that this study can provide useful insights for the future development of LVLMs, helping researchers improve LVLMs for both general and specialized applications.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2407.01312.pdf' target='_blank'>https://arxiv.org/pdf/2407.01312.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Liang, Zhiguang Hu, Junjie Huang, Donglin Di, Anyang Su, Lei Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01312">ToCoAD: Two-Stage Contrastive Learning for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current unsupervised anomaly detection approaches perform well on public datasets but struggle with specific anomaly types due to the domain gap between pre-trained feature extractors and target-specific domains. To tackle this issue, this paper presents a two-stage training strategy, called \textbf{ToCoAD}. In the first stage, a discriminative network is trained by using synthetic anomalies in a self-supervised learning manner. This network is then utilized in the second stage to provide a negative feature guide, aiding in the training of the feature extractor through bootstrap contrastive learning. This approach enables the model to progressively learn the distribution of anomalies specific to industrial datasets, effectively enhancing its generalizability to various types of anomalies. Extensive experiments are conducted to demonstrate the effectiveness of our proposed two-stage training strategy, and our model produces competitive performance, achieving pixel-level AUROC scores of 98.21\%, 98.43\% and 97.70\% on MVTec AD, VisA and BTAD respectively.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2307.13925.pdf' target='_blank'>https://arxiv.org/pdf/2307.13925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruitao Chen, Guoyang Xie, Jiaqi Liu, Jinbao Wang, Ziqi Luo, Jinfan Wang, Feng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13925">EasyNet: An Easy Network for 3D Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D anomaly detection is an emerging and vital computer vision task in industrial manufacturing (IM). Recently many advanced algorithms have been published, but most of them cannot meet the needs of IM. There are several disadvantages: i) difficult to deploy on production lines since their algorithms heavily rely on large pre-trained models; ii) hugely increase storage overhead due to overuse of memory banks; iii) the inference speed cannot be achieved in real-time. To overcome these issues, we propose an easy and deployment-friendly network (called EasyNet) without using pre-trained models and memory banks: firstly, we design a multi-scale multi-modality feature encoder-decoder to accurately reconstruct the segmentation maps of anomalous regions and encourage the interaction between RGB images and depth images; secondly, we adopt a multi-modality anomaly segmentation network to achieve a precise anomaly map; thirdly, we propose an attention-based information entropy fusion module for feature fusion during inference, making it suitable for real-time deployment. Extensive experiments show that EasyNet achieves an anomaly detection AUROC of 92.6% without using pre-trained models and memory banks. In addition, EasyNet is faster than existing methods, with a high frame rate of 94.55 FPS on a Tesla V100 GPU.
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2303.05768.pdf' target='_blank'>https://arxiv.org/pdf/2303.05768.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiming Yao, Wenyong Yu, Wei Luo, Zhenfeng Qiang, Donghao Luo, Xiaotian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05768">Learning Global-Local Correspondence with Semantic Bottleneck for Logical Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel framework, named Global-Local Correspondence Framework (GLCF), for visual anomaly detection with logical constraints. Visual anomaly detection has become an active research area in various real-world applications, such as industrial anomaly detection and medical disease diagnosis. However, most existing methods focus on identifying local structural degeneration anomalies and often fail to detect high-level functional anomalies that involve logical constraints. To address this issue, we propose a two-branch approach that consists of a local branch for detecting structural anomalies and a global branch for detecting logical anomalies. To facilitate local-global feature correspondence, we introduce a novel semantic bottleneck enabled by the visual Transformer. Moreover, we develop feature estimation networks for each branch separately to detect anomalies. Our proposed framework is validated using various benchmarks, including industrial datasets, Mvtec AD, Mvtec Loco AD, and the Retinal-OCT medical dataset. Experimental results show that our method outperforms existing methods, particularly in detecting logical anomalies.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2503.14910.pdf' target='_blank'>https://arxiv.org/pdf/2503.14910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyi Liao, Xun Xu, Yongyi Su, Rong-Cheng Tu, Yifan Liu, Dacheng Tao, Xulei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.14910">Robust Distribution Alignment for Industrial Anomaly Detection under Distribution Shift</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection plays a crucial role in quality control for industrial applications. However, ensuring robustness under unseen domain shifts such as lighting variations or sensor drift remains a significant challenge. Existing methods attempt to address domain shifts by training generalizable models but often rely on prior knowledge of target distributions and can hardly generalise to backbones designed for other data modalities. To overcome these limitations, we build upon memory-bank-based anomaly detection methods, optimizing a robust Sinkhorn distance on limited target training data to enhance generalization to unseen target domains. We evaluate the effectiveness on both 2D and 3D anomaly detection benchmarks with simulated distribution shifts. Our proposed method demonstrates superior results compared with state-of-the-art anomaly detection and domain adaptation methods.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2402.19145.pdf' target='_blank'>https://arxiv.org/pdf/2402.19145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenghao Li, Lei Qi, Xin Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19145">A SAM-guided Two-stream Lightweight Model for Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In industrial anomaly detection, model efficiency and mobile-friendliness become the primary concerns in real-world applications. Simultaneously, the impressive generalization capabilities of Segment Anything (SAM) have garnered broad academic attention, making it an ideal choice for localizing unseen anomalies and diverse real-world patterns. In this paper, considering these two critical factors, we propose a SAM-guided Two-stream Lightweight Model for unsupervised anomaly detection (STLM) that not only aligns with the two practical application requirements but also harnesses the robust generalization capabilities of SAM. We employ two lightweight image encoders, i.e., our two-stream lightweight module, guided by SAM's knowledge. To be specific, one stream is trained to generate discriminative and general feature representations in both normal and anomalous regions, while the other stream reconstructs the same images without anomalies, which effectively enhances the differentiation of two-stream representations when facing anomalous regions. Furthermore, we employ a shared mask decoder and a feature aggregation module to generate anomaly maps. Our experiments conducted on MVTec AD benchmark show that STLM, with about 16M parameters and achieving an inference time in 20ms, competes effectively with state-of-the-art methods in terms of performance, 98.26% on pixel-level AUC and 94.92% on PRO. We further experiment on more difficult datasets, e.g., VisA and DAGM, to demonstrate the effectiveness and generalizability of STLM.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2307.03101.pdf' target='_blank'>https://arxiv.org/pdf/2307.03101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Zhang, Masanori Suganuma, Takayuki Okatani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.03101">Contextual Affinity Distillation for Image Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous works on unsupervised industrial anomaly detection mainly focus on local structural anomalies such as cracks and color contamination. While achieving significantly high detection performance on this kind of anomaly, they are faced with logical anomalies that violate the long-range dependencies such as a normal object placed in the wrong position. In this paper, based on previous knowledge distillation works, we propose to use two students (local and global) to better mimic the teacher's behavior. The local student, which is used in previous studies mainly focuses on structural anomaly detection while the global student pays attention to logical anomalies. To further encourage the global student's learning to capture long-range dependencies, we design the global context condensing block (GCCB) and propose a contextual affinity loss for the student training and anomaly scoring. Experimental results show the proposed method doesn't need cumbersome training techniques and achieves a new state-of-the-art performance on the MVTec LOCO AD dataset.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2503.11088.pdf' target='_blank'>https://arxiv.org/pdf/2503.11088.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Liu, Xun Xu, Shijie Li, Jingyi Liao, Xulei Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11088">Multi-View Industrial Anomaly Detection with Epipolar Constrained Cross-View Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-camera systems provide richer contextual information for industrial anomaly detection. However, traditional methods process each view independently, disregarding the complementary information across viewpoints. Existing multi-view anomaly detection approaches typically employ data-driven cross-view attention for feature fusion but fail to leverage the unique geometric properties of multi-camera setups. In this work, we introduce an epipolar geometry-constrained attention module to guide cross-view fusion, ensuring more effective information aggregation. To further enhance the potential of cross-view attention, we propose a pretraining strategy inspired by memory bank-based anomaly detection. This approach encourages normal feature representations to form multiple local clusters and incorporate multi-view aware negative sample synthesis to regularize pretraining. We demonstrate that our epipolar guided multi-view anomaly detection framework outperforms existing methods on the state-of-the-art multi-view anomaly detection dataset.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2506.16050.pdf' target='_blank'>https://arxiv.org/pdf/2506.16050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Yu, Jieji Ren, Yang Chang, Qiaojun Yu, Xuan Tong, Boyang Wang, Yan Song, You Li, Xinji Mai, Wenqiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16050">Noise Fusion-based Distillation Learning for Anomaly Detection in Complex Industrial Environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection and localization in automated industrial manufacturing can significantly enhance production efficiency and product quality. Existing methods are capable of detecting surface defects in pre-defined or controlled imaging environments. However, accurately detecting workpiece defects in complex and unstructured industrial environments with varying views, poses and illumination remains challenging. We propose a novel anomaly detection and localization method specifically designed to handle inputs with perturbative patterns. Our approach introduces a new framework based on a collaborative distillation heterogeneous teacher network (HetNet), an adaptive local-global feature fusion module, and a local multivariate Gaussian noise generation module. HetNet can learn to model the complex feature distribution of normal patterns using limited information about local disruptive changes. We conducted extensive experiments on mainstream benchmarks. HetNet demonstrates superior performance with approximately 10% improvement across all evaluation metrics on MSC-AD under industrial conditions, while achieving state-of-the-art results on other datasets, validating its resilience to environmental fluctuations and its capability to enhance the reliability of industrial anomaly detection systems across diverse scenarios. Tests in real-world environments further confirm that HetNet can be effectively integrated into production lines to achieve robust and real-time anomaly detection. Codes, images and videos are published on the project website at: https://zihuatanejoyu.github.io/HetNet/
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2408.04839.pdf' target='_blank'>https://arxiv.org/pdf/2408.04839.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanpu Cao, Lu Lin, Jinghui Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.04839">Adversarially Robust Industrial Anomaly Detection Through Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based industrial anomaly detection models have achieved remarkably high accuracy on commonly used benchmark datasets. However, the robustness of those models may not be satisfactory due to the existence of adversarial examples, which pose significant threats to the practical deployment of deep anomaly detectors. Recently, it has been shown that diffusion models can be used to purify the adversarial noises and thus build a robust classifier against adversarial attacks. Unfortunately, we found that naively applying this strategy in anomaly detection (i.e., placing a purifier before an anomaly detector) will suffer from a high anomaly miss rate since the purifying process can easily remove both the anomaly signal and the adversarial perturbations, causing the later anomaly detector failed to detect anomalies. To tackle this issue, we explore the possibility of performing anomaly detection and adversarial purification simultaneously. We propose a simple yet effective adversarially robust anomaly detection method, \textit{AdvRAD}, that allows the diffusion model to act both as an anomaly detector and adversarial purifier. We also extend our proposed method for certified robustness to $l_2$ norm bounded perturbations. Through extensive experiments, we show that our proposed method exhibits outstanding (certified) adversarial robustness while also maintaining equally strong anomaly detection performance on par with the state-of-the-art methods on industrial anomaly detection benchmark datasets.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2507.15335.pdf' target='_blank'>https://arxiv.org/pdf/2507.15335.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Aqeel, Federico Leonardi, Francesco Setti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15335">ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial defect detection systems face critical limitations when confined to one-class anomaly detection paradigms, which assume uniform outlier distributions and struggle with data scarcity in realworld manufacturing environments. We present ExDD (Explicit Dual Distribution), a novel framework that transcends these limitations by explicitly modeling dual feature distributions. Our approach leverages parallel memory banks that capture the distinct statistical properties of both normality and anomalous patterns, addressing the fundamental flaw of uniform outlier assumptions. To overcome data scarcity, we employ latent diffusion models with domain-specific textual conditioning, generating in-distribution synthetic defects that preserve industrial context. Our neighborhood-aware ratio scoring mechanism elegantly fuses complementary distance metrics, amplifying signals in regions exhibiting both deviation from normality and similarity to known defect patterns. Experimental validation on KSDD2 demonstrates superior performance (94.2% I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2312.13783.pdf' target='_blank'>https://arxiv.org/pdf/2312.13783.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soopil Kim, Sion An, Philip Chikontwe, Myeongkyun Kang, Ehsan Adeli, Kilian M. Pohl, Sang Hyun Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.13783">Few Shot Part Segmentation Reveals Compositional Logic for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Logical anomalies (LA) refer to data violating underlying logical constraints e.g., the quantity, arrangement, or composition of components within an image. Detecting accurately such anomalies requires models to reason about various component types through segmentation. However, curation of pixel-level annotations for semantic segmentation is both time-consuming and expensive. Although there are some prior few-shot or unsupervised co-part segmentation algorithms, they often fail on images with industrial object. These images have components with similar textures and shapes, and a precise differentiation proves challenging. In this study, we introduce a novel component segmentation model for LA detection that leverages a few labeled samples and unlabeled images sharing logical constraints. To ensure consistent segmentation across unlabeled images, we employ a histogram matching loss in conjunction with an entropy loss. As segmentation predictions play a crucial role, we propose to enhance both local and global sample validity detection by capturing key aspects from visual semantics via three memory banks: class histograms, component composition embeddings and patch-level representations. For effective LA detection, we propose an adaptive scaling strategy to standardize anomaly scores from different memory banks in inference. Extensive experiments on the public benchmark MVTec LOCO AD reveal our method achieves 98.1% AUROC in LA detection vs. 89.6% from competing methods.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2509.25856.pdf' target='_blank'>https://arxiv.org/pdf/2509.25856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Po-Han Huang, Jeng-Lin Li, Po-Hsuan Huang, Ming-Ching Chang, Wei-Chao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25856">PatchEAD: Unifying Industrial Visual Prompting Frameworks for Patch-Exclusive Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection is increasingly relying on foundation models, aiming for strong out-of-distribution generalization and rapid adaptation in real-world deployments. Notably, past studies have primarily focused on textual prompt tuning, leaving the intrinsic visual counterpart fragmented into processing steps specific to each foundation model. We aim to address this limitation by proposing a unified patch-focused framework, Patch-Exclusive Anomaly Detection (PatchEAD), enabling training-free anomaly detection that is compatible with diverse foundation models. The framework constructs visual prompting techniques, including an alignment module and foreground masking. Our experiments show superior few-shot and batch zero-shot performance compared to prior work, despite the absence of textual features. Our study further examines how backbone structure and pretrained characteristics affect patch-similarity robustness, providing actionable guidance for selecting and configuring foundation models for real-world visual inspection. These results confirm that a well-unified patch-only framework can enable quick, calibration-light deployment without the need for carefully engineered textual prompts.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2503.01292.pdf' target='_blank'>https://arxiv.org/pdf/2503.01292.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yurui Pan, Lidong Wang, Yuchao Chen, Wenbing Zhu, Bo Peng, Mingmin Chi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.01292">PA-CLIP: Enhancing Zero-Shot Anomaly Detection through Pseudo-Anomaly Awareness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In industrial anomaly detection (IAD), accurately identifying defects amidst diverse anomalies and under varying imaging conditions remains a significant challenge. Traditional approaches often struggle with high false-positive rates, frequently misclassifying normal shadows and surface deformations as defects, an issue that becomes particularly pronounced in products with complex and intricate surface features. To address these challenges, we introduce PA-CLIP, a zero-shot anomaly detection method that reduces background noise and enhances defect detection through a pseudo-anomaly-based framework. The proposed method integrates a multiscale feature aggregation strategy for capturing detailed global and local information, two memory banks for distinguishing background information, including normal patterns and pseudo-anomalies, from true anomaly features, and a decision-making module designed to minimize false positives caused by environmental variations while maintaining high defect sensitivity. Demonstrated on the MVTec AD and VisA datasets, PA-CLIP outperforms existing zero-shot methods, providing a robust solution for industrial defect detection.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2408.15113.pdf' target='_blank'>https://arxiv.org/pdf/2408.15113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mykhailo Koshil, Tilman Wegener, Detlef Mentrup, Simone Frintrop, Christian Wilms
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15113">AnomalousPatchCore: Exploring the Use of Anomalous Samples in Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual inspection, or industrial anomaly detection, is one of the most common quality control types in manufacturing. The task is to identify the presence of an anomaly given an image, e.g., a missing component on an image of a circuit board, for subsequent manual inspection. While industrial anomaly detection has seen a surge in recent years, most anomaly detection methods still utilize knowledge only from normal samples, failing to leverage the information from the frequently available anomalous samples. Additionally, they heavily rely on very general feature extractors pre-trained on common image classification datasets. In this paper, we address these shortcomings and propose the new anomaly detection system AnomalousPatchCore~(APC) based on a feature extractor fine-tuned with normal and anomalous in-domain samples and a subsequent memory bank for identifying unusual features. To fine-tune the feature extractor in APC, we propose three auxiliary tasks that address the different aspects of anomaly detection~(classification vs. localization) and mitigate the effect of the imbalance between normal and anomalous samples. Our extensive evaluation on the MVTec dataset shows that APC outperforms state-of-the-art systems in detecting anomalies, which is especially important in industrial anomaly detection given the subsequent manual inspection. In detailed ablation studies, we further investigate the properties of our APC.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2401.10266.pdf' target='_blank'>https://arxiv.org/pdf/2401.10266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maryam Ahang, Todd Charter, Mostafa Abbasi, Maziyar Khadivi, Oluwaseyi Ogunfowora, Homayoun Najjaran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10266">Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Condition monitoring is essential for ensuring the safety, reliability, and efficiency of modern industrial systems. With the increasing complexity of industrial processes, artificial intelligence (AI) has emerged as a powerful tool for fault detection and diagnosis, attracting growing interest from both academia and industry. This paper provides a comprehensive overview of intelligent condition monitoring methods, with a particular emphasis on chemical plants and the widely used Tennessee Eastman Process (TEP) benchmark. State-of-the-art machine learning (ML) and deep learning (DL) algorithms are reviewed, highlighting their strengths, limitations, and applicability to industrial fault detection and diagnosis. Special attention is given to key challenges, including imbalanced and unlabeled data, and to strategies by which models can address these issues. Furthermore, comparative analyses of algorithm performance are presented to guide method selection in practical scenarios. This survey is intended to benefit both newcomers and experienced researchers by consolidating fundamental concepts, summarizing recent advances, and outlining open challenges and promising directions for intelligent condition monitoring in industrial plants.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2509.05034.pdf' target='_blank'>https://arxiv.org/pdf/2509.05034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqi Wu, Hanxi Li, Lin Yuanbo Wu, Hao Chen, Deyin Liu, Peng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05034">Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial product inspection is often performed using Anomaly Detection (AD) frameworks trained solely on non-defective samples. Although defective samples can be collected during production, leveraging them usually requires pixel-level annotations, limiting scalability. To address this, we propose ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial anomaly detection. ADClick generates pixel-wise anomaly annotations from only a few user clicks and a brief textual description, enabling precise and efficient labeling that significantly improves AD model performance (e.g., AP = 96.1\% on MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that aligns visual features and textual prompts via a prototype-based approach for anomaly detection and localization. By combining pixel-level priors with language-guided cues, ADClick-Seg achieves state-of-the-art results on the challenging ``Multi-class'' AD task (AP = 80.0\%, PRO = 97.5\%, Pixel-AUROC = 99.1\% on MVTec AD).
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2508.01591.pdf' target='_blank'>https://arxiv.org/pdf/2508.01591.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxi Li, Jingqi Wu, Lin Yuanbo Wu, Mingliang Li, Deyin Liu, Jialie Shen, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01591">Self-Navigated Residual Mamba for Universal Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose Self-Navigated Residual Mamba (SNARM), a novel framework for universal industrial anomaly detection that leverages ``self-referential learning'' within test images to enhance anomaly discrimination. Unlike conventional methods that depend solely on pre-trained features from normal training data, SNARM dynamically refines anomaly detection by iteratively comparing test patches against adaptively selected in-image references. Specifically, we first compute the ``inter-residuals'' features by contrasting test image patches with the training feature bank. Patches exhibiting small-norm residuals (indicating high normality) are then utilized as self-generated reference patches to compute ``intra-residuals'', amplifying discriminative signals. These inter- and intra-residual features are concatenated and fed into a novel Mamba module with multiple heads, which are dynamically navigated by residual properties to focus on anomalous regions. Finally, AD results are obtained by aggregating the outputs of a self-navigated Mamba in an ensemble learning paradigm. Extensive experiments on MVTec AD, MVTec 3D, and VisA benchmarks demonstrate that SNARM achieves state-of-the-art (SOTA) performance, with notable improvements in all metrics, including Image-AUROC, Pixel-AURC, PRO, and AP.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2311.02747.pdf' target='_blank'>https://arxiv.org/pdf/2311.02747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>AndrÃ© Luiz Buarque Vieira e Silva, Francisco SimÃµes, Danny Kowerko, Tobias Schlosser, Felipe Battisti, Veronica Teichrieb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.02747">Attention Modules Improve Image-Level Anomaly Detection for Industrial Inspection: A DifferNet Case Study</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Within (semi-)automated visual industrial inspection, learning-based approaches for assessing visual defects, including deep neural networks, enable the processing of otherwise small defect patterns in pixel size on high-resolution imagery. The emergence of these often rarely occurring defect patterns explains the general need for labeled data corpora. To alleviate this issue and advance the current state of the art in unsupervised visual inspection, this work proposes a DifferNet-based solution enhanced with attention modules: AttentDifferNet. It improves image-level detection and classification capabilities on three visual anomaly detection datasets for industrial inspection: InsPLAD-fault, MVTec AD, and Semiconductor Wafer. In comparison to the state of the art, AttentDifferNet achieves improved results, which are, in turn, highlighted throughout our quali-quantitative study. Our quantitative evaluation shows an average improvement - compared to DifferNet - of 1.77 +/- 0.25 percentage points in overall AUROC considering all three datasets, reaching SOTA results in InsPLAD-fault, an industrial inspection in-the-wild dataset. As our variants to AttentDifferNet show great prospects in the context of currently investigated approaches, a baseline is formulated, emphasizing the importance of attention for industrial anomaly detection both in the wild and in controlled environments.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2306.02602.pdf' target='_blank'>https://arxiv.org/pdf/2306.02602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Guo, Shuai Lu, Lize Jia, Weihang Zhang, Huiqi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.02602">ReContrast: Domain-Specific Anomaly Detection via Contrastive Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most advanced unsupervised anomaly detection (UAD) methods rely on modeling feature representations of frozen encoder networks pre-trained on large-scale datasets, e.g. ImageNet. However, the features extracted from the encoders that are borrowed from natural image domains coincide little with the features required in the target UAD domain, such as industrial inspection and medical imaging. In this paper, we propose a novel epistemic UAD method, namely ReContrast, which optimizes the entire network to reduce biases towards the pre-trained image domain and orients the network in the target domain. We start with a feature reconstruction approach that detects anomalies from errors. Essentially, the elements of contrastive learning are elegantly embedded in feature reconstruction to prevent the network from training instability, pattern collapse, and identical shortcut, while simultaneously optimizing both the encoder and decoder on the target domain. To demonstrate our transfer ability on various image domains, we conduct extensive experiments across two popular industrial defect detection benchmarks and three medical image UAD tasks, which shows our superiority over current state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2305.17382.pdf' target='_blank'>https://arxiv.org/pdf/2305.17382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuhai Chen, Yue Han, Jiangning Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17382">APRIL-GAN: A Zero-/Few-Shot Anomaly Classification and Segmentation Method for CVPR 2023 VAND Workshop Challenge Tracks 1&2: 1st Place on Zero-shot AD and 4th Place on Few-shot AD</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this technical report, we briefly introduce our solution for the Zero/Few-shot Track of the Visual Anomaly and Novelty Detection (VAND) 2023 Challenge. For industrial visual inspection, building a single model that can be rapidly adapted to numerous categories without or with only a few normal reference images is a promising research direction. This is primarily because of the vast variety of the product types. For the zero-shot track, we propose a solution based on the CLIP model by adding extra linear layers. These layers are used to map the image features to the joint embedding space, so that they can compare with the text features to generate the anomaly maps. Besides, when the reference images are available, we utilize multiple memory banks to store their features and compare them with the features of the test images during the testing phase. In this challenge, our method achieved first place in the zero-shot track, especially excelling in segmentation with an impressive F1 score improvement of 0.0489 over the second-ranked participant. Furthermore, in the few-shot track, we secured the fourth position overall, with our classification F1 score of 0.8687 ranking first among all participating teams.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2303.17354.pdf' target='_blank'>https://arxiv.org/pdf/2303.17354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenping Jin, Fei Guo, Li Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17354">ISSTAD: Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of machine learning, the study of anomaly detection and localization within image data has gained substantial traction, particularly for practical applications such as industrial defect detection. While the majority of existing methods predominantly use Convolutional Neural Networks (CNN) as their primary network architecture, we introduce a novel approach based on the Transformer backbone network. Our method employs a two-stage incremental learning strategy. During the first stage, we train a Masked Autoencoder (MAE) model solely on normal images. In the subsequent stage, we apply pixel-level data augmentation techniques to generate corrupted normal images and their corresponding pixel labels. This process allows the model to learn how to repair corrupted regions and classify the status of each pixel. Ultimately, the model generates a pixel reconstruction error matrix and a pixel anomaly probability matrix. These matrices are then combined to produce an anomaly scoring matrix that effectively detects abnormal regions. When benchmarked against several state-of-the-art CNN-based methods, our approach exhibits superior performance on the MVTec AD dataset, achieving an impressive 97.6% AUC.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2508.03143.pdf' target='_blank'>https://arxiv.org/pdf/2508.03143.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanshu Wang, Xichen Xu, Xiaoning Lei, Guoyang Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03143">SARD: Segmentation-Aware Anomaly Synthesis via Region-Constrained Diffusion with Discriminative Mask Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthesizing realistic and spatially precise anomalies is essential for enhancing the robustness of industrial anomaly detection systems. While recent diffusion-based methods have demonstrated strong capabilities in modeling complex defect patterns, they often struggle with spatial controllability and fail to maintain fine-grained regional fidelity. To overcome these limitations, we propose SARD (Segmentation-Aware anomaly synthesis via Region-constrained Diffusion with discriminative mask Guidance), a novel diffusion-based framework specifically designed for anomaly generation. Our approach introduces a Region-Constrained Diffusion (RCD) process that preserves the background by freezing it and selectively updating only the foreground anomaly regions during the reverse denoising phase, thereby effectively reducing background artifacts. Additionally, we incorporate a Discriminative Mask Guidance (DMG) module into the discriminator, enabling joint evaluation of both global realism and local anomaly fidelity, guided by pixel-level masks. Extensive experiments on the MVTec-AD and BTAD datasets show that SARD surpasses existing methods in segmentation accuracy and visual quality, setting a new state-of-the-art for pixel-level anomaly synthesis.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2506.21398.pdf' target='_blank'>https://arxiv.org/pdf/2506.21398.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Tian, Yufei Li, Yuyang Dai, Wenchao Chen, Xiyang Liu, Bo Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21398">FastRef:Fast Prototype Refinement for Few-Shot Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Few-shot industrial anomaly detection (FS-IAD) presents a critical challenge for practical automated inspection systems operating in data-scarce environments. While existing approaches predominantly focus on deriving prototypes from limited normal samples, they typically neglect to systematically incorporate query image statistics to enhance prototype representativeness. To address this issue, we propose FastRef, a novel and efficient prototype refinement framework for FS-IAD. Our method operates through an iterative two-stage process: (1) characteristic transfer from query features to prototypes via an optimizable transformation matrix, and (2) anomaly suppression through prototype alignment. The characteristic transfer is achieved through linear reconstruction of query features from prototypes, while the anomaly suppression addresses a key observation in FS-IAD that unlike conventional IAD with abundant normal prototypes, the limited-sample setting makes anomaly reconstruction more probable. Therefore, we employ optimal transport (OT) for non-Gaussian sampled features to measure and minimize the gap between prototypes and their refined counterparts for anomaly suppression. For comprehensive evaluation, we integrate FastRef with three competitive prototype-based FS-IAD methods: PatchCore, FastRecon, WinCLIP, and AnomalyDINO. Extensive experiments across four benchmark datasets of MVTec, ViSA, MPDD and RealIAD demonstrate both the effectiveness and computational efficiency of our approach under 1/2/4-shots.
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2504.12749.pdf' target='_blank'>https://arxiv.org/pdf/2504.12749.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijia Li, Guanglei Chu, Jiong Chen, Guo-Sen Xie, Caifeng Shan, Fang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12749">LAD-Reasoner: Tiny Multimodal Models are Good Reasoners for Logical Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in industrial anomaly detection have highlighted the need for deeper logical anomaly analysis, where unexpected relationships among objects, counts, and spatial configurations must be identified and explained. Existing approaches often rely on large-scale external reasoning modules or elaborate pipeline designs, hindering practical deployment and interpretability. To address these limitations, we introduce a new task, Reasoning Logical Anomaly Detection (RLAD), which extends traditional anomaly detection by incorporating logical reasoning. We propose a new framework, LAD-Reasoner, a customized tiny multimodal language model built on Qwen2.5-VL 3B. Our approach leverages a two-stage training paradigm that first employs Supervised Fine-Tuning (SFT) for fine-grained visual understanding, followed by Group Relative Policy Optimization (GRPO) to refine logical anomaly detection and enforce coherent, human-readable reasoning. Crucially, reward signals are derived from both the detection accuracy and the structural quality of the outputs, obviating the need for building chain of thought (CoT) reasoning data. Experiments on the MVTec LOCO AD dataset show that LAD-Reasoner, though significantly smaller, matches the performance of Qwen2.5-VL-72B in accuracy and F1 score, and further excels in producing concise and interpretable rationales. This unified design reduces reliance on large models and complex pipelines, while offering transparent and interpretable insights into logical anomaly detection. Code and data will be released.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2502.11629.pdf' target='_blank'>https://arxiv.org/pdf/2502.11629.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hans-Martin Heyn, Yufei Mao, Roland Weiss, Eric Knauss
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11629">Causal Models in Requirement Specifications for Machine Learning: A vision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Specifying data requirements for machine learning (ML) software systems remains a challenge in requirements engineering (RE). This vision paper explores causal modelling as an RE activity that allows the systematic integration of prior domain knowledge into the design of ML software systems. We propose a workflow to elicit low-level model and data requirements from high-level prior knowledge using causal models. The approach is demonstrated on an industrial fault detection system. This paper outlines future research needed to establish causal modelling as an RE practice.
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2509.25659.pdf' target='_blank'>https://arxiv.org/pdf/2509.25659.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Po-Heng Chou, Chun-Chi Wang, Wei-Lung Mao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25659">YOLO-Based Defect Detection for Metal Sheets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a YOLO-based deep learning (DL) model for automatic defect detection to solve the time-consuming and labor-intensive tasks in industrial manufacturing. In our experiments, the images of metal sheets are used as the dataset for training the YOLO model to detect the defects on the surfaces and in the holes of metal sheets. However, the lack of metal sheet images significantly degrades the performance of detection accuracy. To address this issue, the ConSinGAN is used to generate a considerable amount of data. Four versions of the YOLO model (i.e., YOLOv3, v4, v7, and v9) are combined with the ConSinGAN for data augmentation. The proposed YOLOv9 model with ConSinGAN outperforms the other YOLO models with an accuracy of 91.3%, and a detection time of 146 ms. The proposed YOLOv9 model is integrated into manufacturing hardware and a supervisory control and data acquisition (SCADA) system to establish a practical automated optical inspection (AOI) system. Additionally, the proposed automated defect detection is easily applied to other components in industrial manufacturing.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2508.11550.pdf' target='_blank'>https://arxiv.org/pdf/2508.11550.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zuo Zuo, Jiahao Dong, Yanyun Qu, Zongze Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11550">Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection (AD) plays a significant role in manufacturing where a long-standing challenge is data scarcity. A growing body of works have emerged to address insufficient anomaly data via anomaly generation. However, these anomaly generation methods suffer from lack of fidelity or need to be trained with extra data. To this end, we propose a training-free anomaly generation framework dubbed AAG, which is based on Stable Diffusion (SD)'s strong generation ability for effective anomaly image generation. Given a normal image, mask and a simple text prompt, AAG can generate realistic and natural anomalies in the specific regions and simultaneously keep contents in other regions unchanged. In particular, we propose Cross-Attention Enhancement (CAE) to re-engineer the cross-attention mechanism within Stable Diffusion based on the given mask. CAE increases the similarity between visual tokens in specific regions and text embeddings, which guides these generated visual tokens in accordance with the text description. Besides, generated anomalies need to be more natural and plausible with object in given image. We propose Self-Attention Enhancement (SAE) which improves similarity between each normal visual token and anomaly visual tokens. SAE ensures that generated anomalies are coherent with original pattern. Extensive experiments on MVTec AD and VisA datasets demonstrate effectiveness of AAG in anomaly generation and its utility. Furthermore, anomaly images generated by AAG can bolster performance of various downstream anomaly inspection tasks.
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2505.02626.pdf' target='_blank'>https://arxiv.org/pdf/2505.02626.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sassan Mokhtar, Arian Mousakhan, Silvio Galesso, Jawad Tayyub, Thomas Brox
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02626">Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in visual industrial anomaly detection have demonstrated exceptional performance in identifying and segmenting anomalous regions while maintaining fast inference speeds. However, anomaly classification-distinguishing different types of anomalies-remains largely unexplored despite its critical importance in real-world inspection tasks. To address this gap, we propose VELM, a novel LLM-based pipeline for anomaly classification. Given the critical importance of inference speed, we first apply an unsupervised anomaly detection method as a vision expert to assess the normality of an observation. If an anomaly is detected, the LLM then classifies its type. A key challenge in developing and evaluating anomaly classification models is the lack of precise annotations of anomaly classes in existing datasets. To address this limitation, we introduce MVTec-AC and VisA-AC, refined versions of the widely used MVTec-AD and VisA datasets, which include accurate anomaly class labels for rigorous evaluation. Our approach achieves a state-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD, exceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the effectiveness of VELM in understanding and categorizing anomalies. We hope our methodology and benchmark inspire further research in anomaly classification, helping bridge the gap between detection and comprehensive anomaly characterization.
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2507.23712.pdf' target='_blank'>https://arxiv.org/pdf/2507.23712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aymane Abdali, Bartosz Boguslawski, Lucas Drumetz, Vincent Gripon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23712">Anomalous Samples for Few-Shot Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Several anomaly detection and classification methods rely on large amounts of non-anomalous or "normal" samples under the assump- tion that anomalous data is typically harder to acquire. This hypothesis becomes questionable in Few-Shot settings, where as little as one anno- tated sample can make a significant difference. In this paper, we tackle the question of utilizing anomalous samples in training a model for bi- nary anomaly classification. We propose a methodology that incorporates anomalous samples in a multi-score anomaly detection score leveraging recent Zero-Shot and memory-based techniques. We compare the utility of anomalous samples to that of regular samples and study the benefits and limitations of each. In addition, we propose an augmentation-based validation technique to optimize the aggregation of the different anomaly scores and demonstrate its effectiveness on popular industrial anomaly detection datasets.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2412.00890.pdf' target='_blank'>https://arxiv.org/pdf/2412.00890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Qian, Tianyu Sun, Wenhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.00890">Exploring Large Vision-Language Models for Robust and Efficient Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection (IAD) plays a crucial role in the maintenance and quality control of manufacturing processes. In this paper, we propose a novel approach, Vision-Language Anomaly Detection via Contrastive Cross-Modal Training (CLAD), which leverages large vision-language models (LVLMs) to improve both anomaly detection and localization in industrial settings. CLAD aligns visual and textual features into a shared embedding space using contrastive learning, ensuring that normal instances are grouped together while anomalies are pushed apart. Through extensive experiments on two benchmark industrial datasets, MVTec-AD and VisA, we demonstrate that CLAD outperforms state-of-the-art methods in both image-level anomaly detection and pixel-level anomaly localization. Additionally, we provide ablation studies and human evaluation to validate the importance of key components in our method. Our approach not only achieves superior performance but also enhances interpretability by accurately localizing anomalies, making it a promising solution for real-world industrial applications.
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2401.10050.pdf' target='_blank'>https://arxiv.org/pdf/2401.10050.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyungmin Kim, Donghun Kim, Pyunghwan Ahn, Sungho Suh, Hansang Cho, Junmo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10050">ContextMix: A context-aware data augmentation method for industrial visual inspection systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While deep neural networks have achieved remarkable performance, data augmentation has emerged as a crucial strategy to mitigate overfitting and enhance network performance. These techniques hold particular significance in industrial manufacturing contexts. Recently, image mixing-based methods have been introduced, exhibiting improved performance on public benchmark datasets. However, their application to industrial tasks remains challenging. The manufacturing environment generates massive amounts of unlabeled data on a daily basis, with only a few instances of abnormal data occurrences. This leads to severe data imbalance. Thus, creating well-balanced datasets is not straightforward due to the high costs associated with labeling. Nonetheless, this is a crucial step for enhancing productivity. For this reason, we introduce ContextMix, a method tailored for industrial applications and benchmark datasets. ContextMix generates novel data by resizing entire images and integrating them into other images within the batch. This approach enables our method to learn discriminative features based on varying sizes from resized images and train informative secondary features for object recognition using occluded images. With the minimal additional computation cost of image resizing, ContextMix enhances performance compared to existing augmentation techniques. We evaluate its effectiveness across classification, detection, and segmentation tasks using various network architectures on public benchmark datasets. Our proposed method demonstrates improved results across a range of robustness tasks. Its efficacy in real industrial environments is particularly noteworthy, as demonstrated using the passive component dataset.
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2506.23577.pdf' target='_blank'>https://arxiv.org/pdf/2506.23577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanning Hou, Yanran Ruan, Junfa Li, Shanshan Wang, Jianfeng Qiu, Ke Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23577">StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Enhancing the alignment between text and image features in the CLIP model is a critical challenge in zero-shot industrial anomaly detection tasks. Recent studies predominantly utilize specific category prompts during pretraining, which can cause overfitting to the training categories and limit model generalization. To address this, we propose a method that transforms category names through multicategory name stacking to create stacked prompts, forming the basis of our StackCLIP model. Our approach introduces two key components. The Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts by stacking semantically analogous categories, while utilizing multi-object textual feature fusion to amplify discriminative anomalies among similar objects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific linear layers tailored for each stack cluster and adaptively integrates them based on the attributes of test categories. These modules work together to deliver superior training speed, stability, and convergence, significantly boosting anomaly segmentation performance. Additionally, our stacked prompt framework offers robust generalization across classification tasks. To further improve performance, we introduce the Regulating Prompt Learning (RPL) module, which leverages the generalization power of stacked prompts to refine prompt learning, elevating results in anomaly detection classification tasks. Extensive testing on seven industrial anomaly detection datasets demonstrates that our method achieves state-of-the-art performance in both zero-shot anomaly detection and segmentation tasks.
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2506.05026.pdf' target='_blank'>https://arxiv.org/pdf/2506.05026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oliver Krumpek, Oliver Heimann, JÃ¶rg KrÃ¼ger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05026">Physical Annotation for Automated Optical Inspection: A Concept for In-Situ, Pointer-Based Training Data Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel physical annotation system designed to generate training data for automated optical inspection. The system uses pointer-based in-situ interaction to transfer the valuable expertise of trained inspection personnel directly into a machine learning (ML) training pipeline. Unlike conventional screen-based annotation methods, our system captures physical trajectories and contours directly on the object, providing a more intuitive and efficient way to label data. The core technology uses calibrated, tracked pointers to accurately record user input and transform these spatial interactions into standardised annotation formats that are compatible with open-source annotation software. Additionally, a simple projector-based interface projects visual guidance onto the object to assist users during the annotation process, ensuring greater accuracy and consistency. The proposed concept bridges the gap between human expertise and automated data generation, enabling non-IT experts to contribute to the ML training pipeline and preventing the loss of valuable training samples. Preliminary evaluation results confirm the feasibility of capturing detailed annotation trajectories and demonstrate that integration with CVAT streamlines the workflow for subsequent ML tasks. This paper details the system architecture, calibration procedures and interface design, and discusses its potential contribution to future ML data generation for automated optical inspection.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2505.16487.pdf' target='_blank'>https://arxiv.org/pdf/2505.16487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junqing Chen, Haibo Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16487">Implicit Neural Shape Optimization for 3D High-Contrast Electrical Impedance Tomography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel implicit neural shape optimization framework for 3D high-contrast Electrical Impedance Tomography (EIT), addressing scenarios where conductivity exhibits sharp discontinuities across material interfaces. These high-contrast cases, prevalent in metallic implant monitoring and industrial defect detection, challenge traditional reconstruction methods due to severe ill-posedness. Our approach synergizes shape optimization with implicit neural representations, introducing key innovations including a shape derivative-based optimization scheme that explicitly incorporates high-contrast interface conditions and an efficient latent space representation that reduces variable dimensionality. Through rigorous theoretical analysis of algorithm convergence and extensive numerical experiments, we demonstrate substantial performance improvements, establishing our framework as promising for practical applications in medical imaging with metallic implants and industrial non-destructive testing.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2505.03412.pdf' target='_blank'>https://arxiv.org/pdf/2505.03412.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Bai, Jie Wang, Gaomin Li, Xuan Li, Xiaohu Zhang, Xia Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.03412">CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Internal defect detection constitutes a critical process in ensuring component quality, for which anomaly detection serves as an effective solution. However, existing anomaly detection datasets predominantly focus on surface defects in visible-light images, lacking publicly available X-ray datasets targeting internal defects in components. To address this gap, we construct the first publicly accessible component X-ray anomaly detection (CXR-AD) dataset, comprising real-world X-ray images. The dataset covers five industrial component categories, including 653 normal samples and 561 defect samples with precise pixel-level mask annotations. We systematically analyze the dataset characteristics and identify three major technical challenges: (1) strong coupling between complex internal structures and defect regions, (2) inherent low contrast and high noise interference in X-ray imaging, and (3) significant variations in defect scales and morphologies. To evaluate dataset complexity, we benchmark three state-of-the-art anomaly detection frameworks (feature-based, reconstruction-based, and zero-shot learning methods). Experimental results demonstrate a 29.78% average performance degradation on CXR-AD compared to MVTec AD, highlighting the limitations of current algorithms in handling internal defect detection tasks. To the best of our knowledge, CXR-AD represents the first publicly available X-ray dataset for component anomaly detection, providing a real-world industrial benchmark to advance algorithm development and enhance precision in internal defect inspection technologies.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2504.03306.pdf' target='_blank'>https://arxiv.org/pdf/2504.03306.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mathis Kruse, Bodo Rosenhahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03306">Multi-Flow: Multi-View-Enriched Normalizing Flows for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With more well-performing anomaly detection methods proposed, many of the single-view tasks have been solved to a relatively good degree. However, real-world production scenarios often involve complex industrial products, whose properties may not be fully captured by one single image. While normalizing flow based approaches already work well in single-camera scenarios, they currently do not make use of the priors in multi-view data. We aim to bridge this gap by using these flow-based models as a strong foundation and propose Multi-Flow, a novel multi-view anomaly detection method. Multi-Flow makes use of a novel multi-view architecture, whose exact likelihood estimation is enhanced by fusing information across different views. For this, we propose a new cross-view message-passing scheme, letting information flow between neighboring views. We empirically validate it on the real-world multi-view data set Real-IAD and reach a new state-of-the-art, surpassing current baselines in both image-wise and sample-wise anomaly detection tasks.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2503.20516.pdf' target='_blank'>https://arxiv.org/pdf/2503.20516.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahya Nikouei, Bita Baroutian, Shahabedin Nabavi, Fateme Taraghi, Atefe Aghaei, Ayoob Sajedi, Mohsen Ebrahimi Moghaddam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20516">Small Object Detection: A Comprehensive Survey on Challenges, Techniques and Real-World Applications</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Small object detection (SOD) is a critical yet challenging task in computer vision, with applications like spanning surveillance, autonomous systems, medical imaging, and remote sensing. Unlike larger objects, small objects contain limited spatial and contextual information, making accurate detection difficult. Challenges such as low resolution, occlusion, background interference, and class imbalance further complicate the problem. This survey provides a comprehensive review of recent advancements in SOD using deep learning, focusing on articles published in Q1 journals during 2024-2025. We analyzed challenges, state-of-the-art techniques, datasets, evaluation metrics, and real-world applications. Recent advancements in deep learning have introduced innovative solutions, including multi-scale feature extraction, Super-Resolution (SR) techniques, attention mechanisms, and transformer-based architectures. Additionally, improvements in data augmentation, synthetic data generation, and transfer learning have addressed data scarcity and domain adaptation issues. Furthermore, emerging trends such as lightweight neural networks, knowledge distillation (KD), and self-supervised learning offer promising directions for improving detection efficiency, particularly in resource-constrained environments like Unmanned Aerial Vehicles (UAV)-based surveillance and edge computing. We also review widely used datasets, along with standard evaluation metrics such as mean Average Precision (mAP) and size-specific AP scores. The survey highlights real-world applications, including traffic monitoring, maritime surveillance, industrial defect detection, and precision agriculture. Finally, we discuss open research challenges and future directions, emphasizing the need for robust domain adaptation techniques, better feature fusion strategies, and real-time performance optimization.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2409.06367.pdf' target='_blank'>https://arxiv.org/pdf/2409.06367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianwu Lei, Bohan Wang, Silin Chen, Shurong Cao, Ningmu Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06367">Texture-AD: An Anomaly Detection Dataset and Benchmark for Real Algorithm Development</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection is a crucial process in industrial manufacturing and has made significant advancements recently. However, there is a large variance between the data used in the development and the data collected by the production environment. Therefore, we present the Texture-AD benchmark based on representative texture-based anomaly detection to evaluate the effectiveness of unsupervised anomaly detection algorithms in real-world applications. This dataset includes images of 15 different cloth, 14 semiconductor wafers and 10 metal plates acquired under different optical schemes. In addition, it includes more than 10 different types of defects produced during real manufacturing processes, such as scratches, wrinkles, color variations and point defects, which are often more difficult to detect than existing datasets. All anomalous areas are provided with pixel-level annotations to facilitate comprehensive evaluation using anomaly detection models. Specifically, to adapt to diverse products in automated pipelines, we present a new evaluation method and results of baseline algorithms. The experimental results show that Texture-AD is a difficult challenge for state-of-the-art algorithms. To our knowledge, Texture-AD is the first dataset to be devoted to evaluating industrial defect detection algorithms in the real world. The dataset is available at https://XXX.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2408.01960.pdf' target='_blank'>https://arxiv.org/pdf/2408.01960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Yan, Qingqing Fang, Wenxi Lv, Qinliang Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01960">AnomalySD: Few-Shot Multi-Class Anomaly Detection with Stable Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection is a critical task in industrial manufacturing, aiming to identify defective parts of products. Most industrial anomaly detection methods assume the availability of sufficient normal data for training. This assumption may not hold true due to the cost of labeling or data privacy policies. Additionally, mainstream methods require training bespoke models for different objects, which incurs heavy costs and lacks flexibility in practice. To address these issues, we seek help from Stable Diffusion (SD) model due to its capability of zero/few-shot inpainting, which can be leveraged to inpaint anomalous regions as normal. In this paper, a few-shot multi-class anomaly detection framework that adopts Stable Diffusion model is proposed, named AnomalySD. To adapt SD to anomaly detection task, we design different hierarchical text descriptions and the foreground mask mechanism for fine-tuning SD. In the inference stage, to accurately mask anomalous regions for inpainting, we propose multi-scale mask strategy and prototype-guided mask strategy to handle diverse anomalous regions. Hierarchical text prompts are also utilized to guide the process of inpainting in the inference stage. The anomaly score is estimated based on inpainting result of all masks. Extensive experiments on the MVTec-AD and VisA datasets demonstrate the superiority of our approach. We achieved anomaly classification and segmentation results of 93.6%/94.8% AUROC on the MVTec-AD dataset and 86.1%/96.5% AUROC on the VisA dataset under multi-class and one-shot settings.
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2406.15396.pdf' target='_blank'>https://arxiv.org/pdf/2406.15396.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jerry Chun-Wei Lin, Pi-Wei Chen, Chao-Chun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15396">Feature Purified Transformer With Cross-level Feature Guiding Decoder For Multi-class OOD and Anomaly Deteciton</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstruction networks are prevalently used in unsupervised anomaly and Out-of-Distribution (OOD) detection due to their independence from labeled anomaly data. However, in multi-class datasets, the effectiveness of anomaly detection is often compromised by the models' generalized reconstruction capabilities, which allow anomalies to blend within the expanded boundaries of normality resulting from the added categories, thereby reducing detection accuracy. We introduce the FUTUREG framework, which incorporates two innovative modules: the Feature Purification Module (FPM) and the CFG Decoder. The FPM constrains the normality boundary within the latent space to effectively filter out anomalous features, while the CFG Decoder uses layer-wise encoder representations to guide the reconstruction of filtered features, preserving fine-grained details. Together, these modules enhance the reconstruction error for anomalies, ensuring high-quality reconstructions for normal samples. Our results demonstrate that FUTUREG achieves state-of-the-art performance in multi-class OOD settings and remains competitive in industrial anomaly detection scenarios.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2403.04932.pdf' target='_blank'>https://arxiv.org/pdf/2403.04932.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>BlaÅ¾ Rolih, Dick Ameln, Ashwin Vaidya, Samet Akcay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04932">Divide and Conquer: High-Resolution Industrial Anomaly Detection via Memory Efficient Tiled Ensemble</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection is an important task within computer vision with a wide range of practical use cases. The small size of anomalous regions in many real-world datasets necessitates processing the images at a high resolution. This frequently poses significant challenges concerning memory consumption during the model training and inference stages, leaving some existing methods impractical for widespread adoption. To overcome this challenge, we present the tiled ensemble approach, which reduces memory consumption by dividing the input images into a grid of tiles and training a dedicated model for each tile location. The tiled ensemble is compatible with any existing anomaly detection model without the need for any modification of the underlying architecture. By introducing overlapping tiles, we utilize the benefits of traditional stacking ensembles, leading to further improvements in anomaly detection capabilities beyond high resolution alone. We perform a comprehensive analysis using diverse underlying architectures, including Padim, PatchCore, FastFlow, and Reverse Distillation, on two standard anomaly detection datasets: MVTec and VisA. Our method demonstrates a notable improvement across setups while remaining within GPU memory constraints, consuming only as much GPU memory as a single model needs to process a single tile.
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2307.04956.pdf' target='_blank'>https://arxiv.org/pdf/2307.04956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Zhang, Runwei Ding, Miaoju Ban, Ge Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.04956">PKU-GoodsAD: A Supermarket Goods Dataset for Unsupervised Anomaly Detection and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual anomaly detection is essential and commonly used for many tasks in the field of computer vision. Recent anomaly detection datasets mainly focus on industrial automated inspection, medical image analysis and video surveillance. In order to broaden the application and research of anomaly detection in unmanned supermarkets and smart manufacturing, we introduce the supermarket goods anomaly detection (GoodsAD) dataset. It contains 6124 high-resolution images of 484 different appearance goods divided into 6 categories. Each category contains several common different types of anomalies such as deformation, surface damage and opened. Anomalies contain both texture changes and structural changes. It follows the unsupervised setting and only normal (defect-free) images are used for training. Pixel-precise ground truth regions are provided for all anomalies. Moreover, we also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods. This initial benchmark indicates that some methods which perform well on the industrial anomaly detection dataset (e.g., MVTec AD), show poor performance on our dataset. This is a comprehensive, multi-object dataset for supermarket goods anomaly detection that focuses on real-world applications.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2305.16713.pdf' target='_blank'>https://arxiv.org/pdf/2305.16713.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeeho Hyun, Sangyun Kim, Giyoung Jeon, Seung Hwan Kim, Kyunghoon Bae, Byung Jun Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.16713">ReConPatch : Contrastive Patch Representation Learning for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection is crucial to the advanced identification of product defects such as incorrect parts, misaligned components, and damages in industrial manufacturing. Due to the rare observations and unknown types of defects, anomaly detection is considered to be challenging in machine learning. To overcome this difficulty, recent approaches utilize the common visual representations pre-trained from natural image datasets and distill the relevant features. However, existing approaches still have the discrepancy between the pre-trained feature and the target data, or require the input augmentation which should be carefully designed, particularly for the industrial dataset. In this paper, we introduce ReConPatch, which constructs discriminative features for anomaly detection by training a linear modulation of patch features extracted from the pre-trained model. ReConPatch employs contrastive representation learning to collect and distribute features in a way that produces a target-oriented and easily separable representation. To address the absence of labeled pairs for the contrastive learning, we utilize two similarity measures between data representations, pairwise and contextual similarities, as pseudo-labels. Our method achieves the state-of-the-art anomaly detection performance (99.72%) for the widely used and challenging MVTec AD dataset. Additionally, we achieved a state-of-the-art anomaly detection performance (95.8%) for the BTAD dataset.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2301.12739.pdf' target='_blank'>https://arxiv.org/pdf/2301.12739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Xia, Weijie Lv, Xing He, Nan Li, Chuanqi Liu, Ning Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.12739">FractalAD: A simple industrial anomaly detection method using fractal anomaly generation and backbone knowledge distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Although industrial anomaly detection (AD) technology has made significant progress in recent years, generating realistic anomalies and learning priors of normal remain challenging tasks. In this study, we propose an end-to-end industrial anomaly detection method called FractalAD. Training samples are obtained by synthesizing fractal images and patches from normal samples. This fractal anomaly generation method is designed to sample the full morphology of anomalies. Moreover, we designed a backbone knowledge distillation structure to extract prior knowledge contained in normal samples. The differences between a teacher and a student model are converted into anomaly attention using a cosine similarity attention module. The proposed method enables an end-to-end semantic segmentation network to be used for anomaly detection without adding any trainable parameters to the backbone and segmentation head, and has obvious advantages over other methods in training and inference speed.. The results of ablation studies confirmed the effectiveness of fractal anomaly generation and backbone knowledge distillation. The results of performance experiments showed that FractalAD achieved competitive results on the MVTec AD dataset and MVTec 3D-AD dataset compared with other state-of-the-art anomaly detection methods.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2508.14504.pdf' target='_blank'>https://arxiv.org/pdf/2508.14504.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bernd Hofmann, Albert Scheck, Joerg Franke, Patrick Bruendl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14504">PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The detection of anomalies in manufacturing processes is crucial to ensure product quality and identify process deviations. Statistical and data-driven approaches remain the standard in industrial anomaly detection, yet their adaptability and usability are constrained by the dependence on extensive annotated datasets and limited flexibility under dynamic production conditions. Recent advances in the perception capabilities of foundation models provide promising opportunities for their adaptation to this downstream task. This paper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novel framework that leverages the multimodal and reasoning capabilities of foundation models for industrial anomaly detection. Specifically, PB-IAD addresses three key requirements of dynamic production environments: data sparsity, agile adaptability, and domain user centricity. In addition to the anomaly detection, the framework includes a prompt template that is specifically designed for iteratively implementing domain-specific process knowledge, as well as a pre-processing module that translates domain user inputs into effective system prompts. This user-centric design allows domain experts to customise the system flexibly without requiring data science expertise. The proposed framework is evaluated by utilizing GPT-4.1 across three distinct manufacturing scenarios, two data modalities, and an ablation study to systematically assess the contribution of semantic instructions. Furthermore, PB-IAD is benchmarked to state-of-the-art methods for anomaly detection such as PatchCore. The results demonstrate superior performance, particularly in data-sparse scenarios and low-shot settings, achieved solely through semantic instructions.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2507.00102.pdf' target='_blank'>https://arxiv.org/pdf/2507.00102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bernd Hofmann, Patrick Bruendl, Huong Giang Nguyen, Joerg Franke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00102">Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring consistent product quality in modern manufacturing is crucial, particularly in safety-critical applications. Conventional quality control approaches, reliant on manually defined thresholds and features, lack adaptability to the complexity and variability inherent in production data and necessitate extensive domain expertise. Conversely, data-driven methods, such as machine learning, demonstrate high detection performance but typically function as black-box models, thereby limiting their acceptance in industrial environments where interpretability is paramount. This paper introduces a methodology for industrial fault detection, which is both data-driven and transparent. The approach integrates a supervised machine learning model for multi-class fault classification, Shapley Additive Explanations for post-hoc interpretability, and a do-main-specific visualisation technique that maps model explanations to operator-interpretable features. Furthermore, the study proposes an evaluation methodology that assesses model explanations through quantitative perturbation analysis and evaluates visualisations by qualitative expert assessment. The approach was applied to the crimping process, a safety-critical joining technique, using a dataset of univariate, discrete time series. The system achieves a fault detection accuracy of 95.9 %, and both quantitative selectivity analysis and qualitative expert evaluations confirmed the relevance and inter-pretability of the generated explanations. This human-centric approach is designed to enhance trust and interpretability in data-driven fault detection, thereby contributing to applied system design in industrial quality control.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2505.19750.pdf' target='_blank'>https://arxiv.org/pdf/2505.19750.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaiyuan Zhang, Hang Chen, Yu Cheng, Shunyi Wu, Linghao Sun, Linao Han, Zeyu Shi, Lei Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19750">SuperAD: A Training-free Anomaly Classification and Segmentation Method for CVPR 2025 VAND 3.0 Workshop Challenge Track 1: Adapt & Detect</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this technical report, we present our solution to the CVPR 2025 Visual Anomaly and Novelty Detection (VAND) 3.0 Workshop Challenge Track 1: Adapt & Detect: Robust Anomaly Detection in Real-World Applications. In real-world industrial anomaly detection, it is crucial to accurately identify anomalies with physical complexity, such as transparent or reflective surfaces, occlusions, and low-contrast contaminations. The recently proposed MVTec AD 2 dataset significantly narrows the gap between publicly available benchmarks and anomalies found in real-world industrial environments. To address the challenges posed by this dataset--such as complex and varying lighting conditions and real anomalies with large scale differences--we propose a fully training-free anomaly detection and segmentation method based on feature extraction using the DINOv2 model named SuperAD. Our method carefully selects a small number of normal reference images and constructs a memory bank by leveraging the strong representational power of DINOv2. Anomalies are then segmented by performing nearest neighbor matching between test image features and the memory bank. Our method achieves competitive results on both test sets of the MVTec AD 2 dataset.
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2504.12856.pdf' target='_blank'>https://arxiv.org/pdf/2504.12856.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifeng Cheng, Juan Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12856">3D-PNAS: 3D Industrial Surface Anomaly Synthesis with Perlin Noise</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large pretrained vision foundation models have shown significant potential in various vision tasks. However, for industrial anomaly detection, the scarcity of real defect samples poses a critical challenge in leveraging these models. While 2D anomaly generation has significantly advanced with established generative models, the adoption of 3D sensors in industrial manufacturing has made leveraging 3D data for surface quality inspection an emerging trend. In contrast to 2D techniques, 3D anomaly generation remains largely unexplored, limiting the potential of 3D data in industrial quality inspection. To address this gap, we propose a novel yet simple 3D anomaly generation method, 3D-PNAS, based on Perlin noise and surface parameterization. Our method generates realistic 3D surface anomalies by projecting the point cloud onto a 2D plane, sampling multi-scale noise values from a Perlin noise field, and perturbing the point cloud along its normal direction. Through comprehensive visualization experiments, we demonstrate how key parameters - including noise scale, perturbation strength, and octaves, provide fine-grained control over the generated anomalies, enabling the creation of diverse defect patterns from pronounced deformations to subtle surface variations. Additionally, our cross-category experiments show that the method produces consistent yet geometrically plausible anomalies across different object types, adapting to their specific surface characteristics. We also provide a comprehensive codebase and visualization toolkit to facilitate future research.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2503.12910.pdf' target='_blank'>https://arxiv.org/pdf/2503.12910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyi Yuan, Chenqiang Gao, Pengyu Jie, Xuan Xia, Shangri Huang, Wanquan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12910">AFR-CLIP: Enhancing Zero-Shot Industrial Anomaly Detection with Stateless-to-Stateful Anomaly Feature Rectification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, zero-shot anomaly detection (ZSAD) has emerged as a pivotal paradigm for industrial inspection and medical diagnostics, detecting defects in novel objects without requiring any target-dataset samples during training. Existing CLIP-based ZSAD methods generate anomaly maps by measuring the cosine similarity between visual and textual features. However, CLIP's alignment with object categories instead of their anomalous states limits its effectiveness for anomaly detection. To address this limitation, we propose AFR-CLIP, a CLIP-based anomaly feature rectification framework. AFR-CLIP first performs image-guided textual rectification, embedding the implicit defect information from the image into a stateless prompt that describes the object category without indicating any anomalous state. The enriched textual embeddings are then compared with two pre-defined stateful (normal or abnormal) embeddings, and their text-on-text similarity yields the anomaly map that highlights defective regions. To further enhance perception to multi-scale features and complex anomalies, we introduce self prompting (SP) and multi-patch feature aggregation (MPFA) modules. Extensive experiments are conducted on eleven anomaly detection benchmarks across industrial and medical domains, demonstrating AFR-CLIP's superiority in ZSAD.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2502.19106.pdf' target='_blank'>https://arxiv.org/pdf/2502.19106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianle Yang, Luyao Chang, Jiadong Yan, Juntao Li, Zhi Wang, Ke Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19106">A Survey on Foundation-Model-Based Industrial Defect Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As industrial products become abundant and sophisticated, visual industrial defect detection receives much attention, including two-dimensional and three-dimensional visual feature modeling. Traditional methods use statistical analysis, abnormal data synthesis modeling, and generation-based models to separate product defect features and complete defect detection. Recently, the emergence of foundation models has brought visual and textual semantic prior knowledge. Many methods are based on foundation models (FM) to improve the accuracy of detection, but at the same time, increase model complexity and slow down inference speed. Some FM-based methods have begun to explore lightweight modeling ways, which have gradually attracted attention and deserve to be systematically analyzed. In this paper, we conduct a systematic survey with comparisons and discussions of foundation model methods from different aspects and briefly review non-foundation model (NFM) methods recently published. Furthermore, we discuss the differences between FM and NFM methods from training objectives, model structure and scale, model performance, and potential directions for future exploration. Through comparison, we find FM methods are more suitable for few-shot and zero-shot learning, which are more in line with actual industrial application scenarios and worthy of in-depth research.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2408.13526.pdf' target='_blank'>https://arxiv.org/pdf/2408.13526.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vahid MohammadZadeh Eivaghi, Mahdi Aliyari Shoorehdeli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13526">Learning a Factorized Orthogonal Latent Space using Encoder-only Architecture for Fault Detection; An Alarm management perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>False and nuisance alarms in industrial fault detection systems are often triggered by uncertainty, causing normal process variable fluctuations to be erroneously identified as faults. This paper introduces a novel encoder-based residual design that effectively decouples the stochastic and deterministic components of process variables without imposing detection delay. The proposed model employs two distinct encoders to factorize the latent space into two orthogonal spaces: one for the deterministic part and the other for the stochastic part. To ensure the identifiability of the desired spaces, constraints are applied during training. The deterministic space is constrained to be smooth to guarantee determinism, while the stochastic space is required to resemble standard Gaussian noise. Additionally, a decorrelation term enforces the independence of the learned representations. The efficacy of this approach is demonstrated through numerical examples and its application to the Tennessee Eastman process, highlighting its potential for robust fault detection. By focusing decision logic solely on deterministic factors, the proposed model significantly enhances prediction quality while achieving nearly zero false alarms and missed detections, paving the way for improved operational safety and integrity in industrial environments.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2405.13571.pdf' target='_blank'>https://arxiv.org/pdf/2405.13571.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Sui, Daniel Lichau, Josselin LefÃ¨vre, Harold Phelippeau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13571">Incomplete Multimodal Industrial Anomaly Detection via Cross-Modal Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies of multimodal industrial anomaly detection (IAD) based on 3D point clouds and RGB images have highlighted the importance of exploiting the redundancy and complementarity among modalities for accurate classification and segmentation. However, achieving multimodal IAD in practical production lines remains a work in progress. It is essential to consider the trade-offs between the costs and benefits associated with the introduction of new modalities while ensuring compatibility with current processes. Existing quality control processes combine rapid in-line inspections, such as optical and infrared imaging with high-resolution but time-consuming near-line characterization techniques, including industrial CT and electron microscopy to manually or semi-automatically locate and analyze defects in the production of Li-ion batteries and composite materials. Given the cost and time limitations, only a subset of the samples can be inspected by all in-line and near-line methods, and the remaining samples are only evaluated through one or two forms of in-line inspection. To fully exploit data for deep learning-driven automatic defect detection, the models must have the ability to leverage multimodal training and handle incomplete modalities during inference. In this paper, we propose CMDIAD, a Cross-Modal Distillation framework for IAD to demonstrate the feasibility of a Multi-modal Training, Few-modal Inference (MTFI) pipeline. Our findings show that the MTFI pipeline can more effectively utilize incomplete multimodal information compared to applying only a single modality for training and inference. Moreover, we investigate the reasons behind the asymmetric performance improvement using point clouds or RGB images as the main modality of inference. This provides a foundation for our future multimodal dataset construction with additional modalities from manufacturing scenarios.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2311.06797.pdf' target='_blank'>https://arxiv.org/pdf/2311.06797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyang Bi, Yueyang Li, Haichi Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.06797">Dual-Branch Reconstruction Network for Industrial Anomaly Detection with RGB-D Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised anomaly detection methods are at the forefront of industrial anomaly detection efforts and have made notable progress. Previous work primarily used 2D information as input, but multi-modal industrial anomaly detection based on 3D point clouds and RGB images is just beginning to emerge. The regular approach involves utilizing large pre-trained models for feature representation and storing them in memory banks. However, the above methods require a longer inference time and higher memory usage, which cannot meet the real-time requirements of the industry. To overcome these issues, we propose a lightweight dual-branch reconstruction network(DBRN) based on RGB-D input, learning the decision boundary between normal and abnormal examples. The requirement for alignment between the two modalities is eliminated by using depth maps instead of point cloud input. Furthermore, we introduce an importance scoring module in the discriminative network to assist in fusing features from these two modalities, thereby obtaining a comprehensive discriminative result. DBRN achieves 92.8% AUROC with high inference efficiency on the MVTec 3D-AD dataset without large pre-trained models and memory banks.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2202.08414.pdf' target='_blank'>https://arxiv.org/pdf/2202.08414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nathan Jessurun, Olivia P. Dizon-Paradis, Jacob Harrison, Shajib Ghosh, Mark M. Tehranipoor, Damon L. Woodard, Navid Asadizanjani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.08414">FPIC: A Novel Semantic Dataset for Optical PCB Assurance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Outsourced printed circuit board (PCB) fabrication necessitates increased hardware assurance capabilities. Several assurance techniques based on automated optical inspection (AOI) have been proposed that leverage PCB images acquired using digital cameras. We review state-of-the-art AOI techniques and observe a strong, rapid trend toward machine learning (ML) solutions. These require significant amounts of labeled ground truth data, which is lacking in the publicly available PCB data space. We contribute the FICS PCB Image Collection (FPIC) dataset to address this need. Additionally, we outline new hardware security methodologies enabled by our data set.
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2510.05903.pdf' target='_blank'>https://arxiv.org/pdf/2510.05903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebastian Höfer, Dorian Henning, Artemij Amiranashvili, Douglas Morrison, Mariliza Tzes, Ingmar Posner, Marc Matvienko, Alessandro Rennola, Anton Milan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05903">Kaputt: A Large-Scale Dataset for Visual Defect Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel large-scale dataset for defect detection in a logistics setting. Recent work on industrial anomaly detection has primarily focused on manufacturing scenarios with highly controlled poses and a limited number of object categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC scores. In contrast to manufacturing, anomaly detection in retail logistics faces new challenges, particularly in the diversity and variability of object pose and appearance. Leading anomaly detection methods fall short when applied to this new setting. To bridge this gap, we introduce a new benchmark that overcomes the current limitations of existing datasets. With over 230,000 images (and more than 29,000 defective instances), it is 40 times larger than MVTec-AD and contains more than 48,000 distinct objects. To validate the difficulty of the problem, we conduct an extensive evaluation of multiple state-of-the-art anomaly detection methods, demonstrating that they do not surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that existing methods struggle to leverage normal samples under heavy pose and appearance variation. With our large-scale dataset, we set a new benchmark and encourage future research towards solving this challenging problem in retail logistics anomaly detection. The dataset is available for download under https://www.kaputt-dataset.com.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2510.05071.pdf' target='_blank'>https://arxiv.org/pdf/2510.05071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Debojyoti Ghosh, Soumya K Ghosh, Adrijit Goswami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05071">Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient and accurate classification of waste and industrial surface defects is essential for ensuring sustainable waste management and maintaining high standards in quality control. This paper introduces the Neuroplastic Modular Classifier, a novel hybrid architecture designed for robust and adaptive image classification in dynamic environments. The model combines a ResNet-50 backbone for localized feature extraction with a Vision Transformer (ViT) to capture global semantic context. Additionally, FAISS-based similarity retrieval is incorporated to provide a memory-like reference to previously encountered data, enriching the model's feature space. A key innovation of our architecture is the neuroplastic modular design composed of expandable, learnable blocks that dynamically grow during training when performance plateaus. Inspired by biological learning systems, this mechanism allows the model to adapt to data complexity over time, improving generalization. Beyond garbage classification, we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2), which involves industrial defect detection on metal surfaces. Experimental results across domains show that the proposed architecture outperforms traditional static models in both accuracy and adaptability. The Neuroplastic Modular Classifier offers a scalable, high-performance solution for real-world image classification, with strong applicability in both environmental and industrial domains.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2508.16034.pdf' target='_blank'>https://arxiv.org/pdf/2508.16034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cory Gardner, Byungseok Min, Tae-Hyuk Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16034">Wavelet-Enhanced PaDiM for Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection and localization in industrial images are essential for automated quality inspection. PaDiM, a prominent method, models the distribution of normal image features extracted by pre-trained Convolutional Neural Networks (CNNs) but reduces dimensionality through random channel selection, potentially discarding structured information. We propose Wavelet-Enhanced PaDiM (WE-PaDiM), which integrates Discrete Wavelet Transform (DWT) analysis with multi-layer CNN features in a structured manner. WE-PaDiM applies 2D DWT to feature maps from multiple backbone layers, selects specific frequency subbands (e.g., LL, LH, HL), spatially aligns them, and concatenates them channel-wise before modeling with PaDiM's multivariate Gaussian framework. This DWT-before-concatenation strategy provides a principled method for feature selection based on frequency content relevant to anomalies, leveraging multi-scale wavelet information as an alternative to random selection. We evaluate WE-PaDiM on the challenging MVTec AD dataset with multiple backbones (ResNet-18 and EfficientNet B0-B6). The method achieves strong performance in anomaly detection and localization, yielding average results of 99.32% Image-AUC and 92.10% Pixel-AUC across 15 categories with per-class optimized configurations. Our analysis shows that wavelet choices affect performance trade-offs: simpler wavelets (e.g., Haar) with detail subbands (HL or LH/HL/HH) often enhance localization, while approximation bands (LL) improve image-level detection. WE-PaDiM thus offers a competitive and interpretable alternative to random feature selection in PaDiM, achieving robust results suitable for industrial inspection with comparable efficiency.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2508.05503.pdf' target='_blank'>https://arxiv.org/pdf/2508.05503.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongwei Ji, Bingzhang Hu, Yi Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05503">AutoIAD: Manager-Driven Multi-Agent Collaboration for Automated Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial anomaly detection (IAD) is critical for manufacturing quality control, but conventionally requires significant manual effort for various application scenarios. This paper introduces AutoIAD, a multi-agent collaboration framework, specifically designed for end-to-end automated development of industrial visual anomaly detection. AutoIAD leverages a Manager-Driven central agent to orchestrate specialized sub-agents (including Data Preparation, Data Loader, Model Designer, Trainer) and integrates a domain-specific knowledge base, which intelligently handles the entire pipeline using raw industrial image data to develop a trained anomaly detection model. We construct a comprehensive benchmark using MVTec AD datasets to evaluate AutoIAD across various LLM backends. Extensive experiments demonstrate that AutoIAD significantly outperforms existing general-purpose agentic collaboration frameworks and traditional AutoML frameworks in task completion rate and model performance (AUROC), while effectively mitigating issues like hallucination through iterative refinement. Ablation studies further confirm the crucial roles of the Manager central agent and the domain knowledge base module in producing robust and high-quality IAD solutions.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2507.07579.pdf' target='_blank'>https://arxiv.org/pdf/2507.07579.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianwei Mu, Feiyu Duan, Bo Zhou, Dan Xue, Manhong Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07579">NexViTAD: Few-shot Unsupervised Cross-Domain Defect Detection via Vision Foundation Models and Multi-Task Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel few-shot cross-domain anomaly detection framework, Nexus Vision Transformer for Anomaly Detection (NexViTAD), based on vision foundation models, which effectively addresses domain-shift challenges in industrial anomaly detection through innovative shared subspace projection mechanisms and multi-task learning (MTL) module. The main innovations include: (1) a hierarchical adapter module that adaptively fuses complementary features from Hiera and DINO-v2 pre-trained models, constructing more robust feature representations; (2) a shared subspace projection strategy that enables effective cross-domain knowledge transfer through bottleneck dimension constraints and skip connection mechanisms; (3) a MTL Decoder architecture supports simultaneous processing of multiple source domains, significantly enhancing model generalization capabilities; (4) an anomaly score inference method based on Sinkhorn-K-means clustering, combined with Gaussian filtering and adaptive threshold processing for precise pixel level. Valuated on the MVTec AD dataset, NexViTAD delivers state-of-the-art performance with an AUC of 97.5%, AP of 70.4%, and PRO of 95.2% in the target domains, surpassing other recent models, marking a transformative advance in cross-domain defect detection.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2506.14521.pdf' target='_blank'>https://arxiv.org/pdf/2506.14521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Korbinian Pfab, Marcel Rothering
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14521">Towards Improved Research Methodologies for Industrial AI: A case study of false call reduction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Are current artificial intelligence (AI) research methodologies ready to create successful, productive, and profitable AI applications? This work presents a case study on an industrial AI use case called false call reduction for automated optical inspection to demonstrate the shortcomings of current best practices. We identify seven weaknesses prevalent in related peer-reviewed work and experimentally show their consequences. We show that the best-practice methodology would fail for this use case. We argue amongst others for the necessity of requirement-aware metrics to ensure achieving business objectives, clear definitions of success criteria, and a thorough analysis of temporal dynamics in experimental datasets. Our work encourages researchers to critically assess their methodologies for more successful applied AI research.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2503.04997.pdf' target='_blank'>https://arxiv.org/pdf/2503.04997.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul J. Krassnig, Dieter P. Gruber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04997">ISP-AD: A Large-Scale Real-World Dataset for Advancing Industrial Anomaly Detection with Synthetic and Real Defects</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic visual inspection using machine learning plays a key role in achieving zero-defect policies in industry. Research on anomaly detection is constrained by the availability of datasets that capture complex defect appearances and imperfect imaging conditions, which are typical of production processes. Recent benchmarks indicate that most publicly available datasets are biased towards optimal imaging conditions, leading to an overestimation of their applicability in real-world industrial scenarios. To address this gap, we introduce the Industrial Screen Printing Anomaly Detection Dataset (ISP-AD). It presents challenging small and weakly contrasted surface defects embedded within structured patterns exhibiting high permitted design variability. To the best of our knowledge, it is the largest publicly available industrial dataset to date, including both synthetic and real defects collected directly from the factory floor. Beyond benchmarking recent unsupervised anomaly detection methods, experiments on a mixed supervised training strategy, incorporating both synthesized and real defects, were conducted. Experiments show that even a small amount of injected, weakly labeled real defects improves generalization. Furthermore, starting from training on purely synthetic defects, emerging real defective samples can be efficiently integrated into subsequent scalable training. Overall, our findings indicate that model-free synthetic defects can provide a cold-start baseline, whereas a small number of injected real defects refine the decision boundary for previously unseen defect characteristics. The presented unsupervised and supervised dataset splits are designed to emphasize research on unsupervised, self-supervised, and supervised approaches, enhancing their applicability to industrial settings.
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2501.11310.pdf' target='_blank'>https://arxiv.org/pdf/2501.11310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdelrahman Alzarooni, Ehtesham Iqbal, Samee Ullah Khan, Sajid Javed, Brain Moyo, Yusra Abdulrahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.11310">Anomaly Detection for Industrial Applications, Its Challenges, Solutions, and Future Directions: A Review</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection from images captured using camera sensors is one of the mainstream applications at the industrial level. Particularly, it maintains the quality and optimizes the efficiency in production processes across diverse industrial tasks, including advanced manufacturing and aerospace engineering. Traditional anomaly detection workflow is based on a manual inspection by human operators, which is a tedious task. Advances in intelligent automated inspection systems have revolutionized the Industrial Anomaly Detection (IAD) process. Recent vision-based approaches can automatically extract, process, and interpret features using computer vision and align with the goals of automation in industrial operations. In light of the shift in inspection methodologies, this survey reviews studies published since 2019, with a specific focus on vision-based anomaly detection. The components of an IAD pipeline that are overlooked in existing surveys are presented, including areas related to data acquisition, preprocessing, learning mechanisms, and evaluation. In addition to the collected publications, several scientific and industry-related challenges and their perspective solutions are highlighted. Popular and relevant industrial datasets are also summarized, providing further insight into inspection applications. Finally, future directions of vision-based IAD are discussed, offering researchers insight into the state-of-the-art of industrial inspection.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2411.16767.pdf' target='_blank'>https://arxiv.org/pdf/2411.16767.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngjae Cho, Gwangyeol Kim, Sirojbek Safarov, Seongdeok Bang, Jaewoo Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16767">Background-Aware Defect Generation for Robust Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting anomalies in industrial settings is challenging due to the scarcity of labeled anomalous data. Generative models can mitigate this issue by synthesizing realistic defect samples, but existing approaches often fail to model the crucial interplay between defects and their background. This oversight leads to unrealistic anomalies, especially in scenarios where contextual consistency is essential (i.e., logical anomaly). To address this, we propose a novel background-aware defect generation framework, where the background influences defect denoising without affecting the background itself by ensuring realistic synthesis while preserving structural integrity. Our method leverages a disentanglement loss to separate the background' s denoising process from the defect, enabling controlled defect synthesis through DDIM Inversion. We theoretically demonstrate that our approach maintains background fidelity while generating contextually accurate defects. Extensive experiments on MVTec AD and MVTec Loco benchmarks validate our mehtod's superiority over existing techniques in both defect generation quality and anomaly detection performance.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2410.10234.pdf' target='_blank'>https://arxiv.org/pdf/2410.10234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunsuke Sakai, Tatushito Hasegawa, Makoto Koshino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10234">LADMIM: Logical Anomaly Detection with Masked Image Modeling in Discrete Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting anomalies such as incorrect combinations of objects or deviations in their positions is a challenging problem in industrial anomaly detection. Traditional methods mainly focus on local features of normal images, such as scratches and dirt, making detecting anomalies in the relationships between features difficult. Masked image modeling(MIM) is a self-supervised learning technique that predicts the feature representation of masked regions in an image. To reconstruct the masked regions, it is necessary to understand how the image is composed, allowing the learning of relationships between features within the image. We propose a novel approach that leverages the characteristics of MIM to detect logical anomalies effectively. To address blurriness in the reconstructed image, we replace pixel prediction with predicting the probability distribution of discrete latent variables of the masked regions using a tokenizer. We evaluated the proposed method on the MVTecLOCO dataset, achieving an average AUC of 0.867, surpassing traditional reconstruction-based and distillation-based methods.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2407.02910.pdf' target='_blank'>https://arxiv.org/pdf/2407.02910.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas BÃ¼hler, Jonas Fehrenbach, Lucas Steinmann, Christian Nauck, Marios Koulakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02910">Domain-independent detection of known anomalies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>One persistent obstacle in industrial quality inspection is the detection of anomalies. In real-world use cases, two problems must be addressed: anomalous data is sparse and the same types of anomalies need to be detected on previously unseen objects. Current anomaly detection approaches can be trained with sparse nominal data, whereas domain generalization approaches enable detecting objects in previously unseen domains. Utilizing those two observations, we introduce the hybrid task of domain generalization on sparse classes. To introduce an accompanying dataset for this task, we present a modification of the well-established MVTec AD dataset by generating three new datasets. In addition to applying existing methods for benchmark, we design two embedding-based approaches, Spatial Embedding MLP (SEMLP) and Labeled PatchCore. Overall, SEMLP achieves the best performance with an average image-level AUROC of 87.2 % vs. 80.4 % by MIRO. The new and openly available datasets allow for further research to improve industrial anomaly detection.
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2406.07694.pdf' target='_blank'>https://arxiv.org/pdf/2406.07694.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Can Akbas, Irem Su Arin, Sinan Onal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.07694">A PRISMA Driven Systematic Review of Publicly Available Datasets for Benchmark and Model Developments for Industrial Defect Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in quality control across various industries have increasingly utilized the integration of video cameras and image processing for effective defect detection. A critical barrier to progress is the scarcity of comprehensive datasets featuring annotated defects, which are essential for developing and refining automated defect detection models. This systematic review, spanning from 2015 to 2023, identifies 15 publicly available datasets and critically examines them to assess their effectiveness and applicability for benchmarking and model development. Our findings reveal a diverse landscape of datasets, such as NEU-CLS, NEU-DET, DAGM, KolektorSDD, PCB Defect Dataset, and the Hollow Cylindrical Defect Detection Dataset, each with unique strengths and limitations in terms of image quality, defect type representation, and real-world applicability. The goal of this systematic review is to consolidate these datasets in a single location, providing researchers who seek such publicly available resources with a comprehensive reference.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2404.13273.pdf' target='_blank'>https://arxiv.org/pdf/2404.13273.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junpu Wang, Guili Xu, Chunlei Li, Guangshuai Gao, Yuehua Cheng, Bing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13273">Multi-feature Reconstruction Network using Crossed-mask Restoration for Unsupervised Industrial Anomaly Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unsupervised anomaly detection using only normal samples is of great significance for quality inspection in industrial manufacturing. Although existing reconstruction-based methods have achieved promising results, they still face two problems: poor distinguishable information in image reconstruction and well abnormal regeneration caused by model under-regularization. To overcome the above issues, we convert the image reconstruction into a combination of parallel feature restorations and propose a multi-feature reconstruction network, MFRNet, using crossed-mask restoration in this paper. Specifically, a multi-scale feature aggregator is first developed to generate more discriminative hierarchical representations of the input images from a pre-trained model. Subsequently, a crossed-mask generator is adopted to randomly cover the extracted feature map, followed by a restoration network based on the transformer structure for high-quality repair of the missing regions. Finally, a hybrid loss is equipped to guide model training and anomaly estimation, which gives consideration to both the pixel and structural similarity. Extensive experiments show that our method is highly competitive with or significantly outperforms other state-of-the-arts on four public available datasets and one self-made dataset.
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2403.06247.pdf' target='_blank'>https://arxiv.org/pdf/2403.06247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyu Lee, Jongwon Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06247">Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a text-guided variational image generation method to address the challenge of getting clean data for anomaly detection in industrial manufacturing. Our method utilizes text information about the target object, learned from extensive text library documents, to generate non-defective data images resembling the input image. The proposed framework ensures that the generated non-defective images align with anticipated distributions derived from textual and image-based knowledge, ensuring stability and generality. Experimental results demonstrate the effectiveness of our approach, surpassing previous methods even with limited non-defective data. Our approach is validated through generalization tests across four baseline models and three distinct datasets. We present an additional analysis to enhance the effectiveness of anomaly detection models by utilizing the generated images.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2309.12700.pdf' target='_blank'>https://arxiv.org/pdf/2309.12700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiangqi Liu, Feng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.12700">mixed attention auto encoder for multi-class industrial anomaly detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most existing methods for unsupervised industrial anomaly detection train a separate model for each object category. This kind of approach can easily capture the category-specific feature distributions, but results in high storage cost and low training efficiency. In this paper, we propose a unified mixed-attention auto encoder (MAAE) to implement multi-class anomaly detection with a single model. To alleviate the performance degradation due to the diverse distribution patterns of different categories, we employ spatial attentions and channel attentions to effectively capture the global category information and model the feature distributions of multiple classes. Furthermore, to simulate the realistic noises on features and preserve the surface semantics of objects from different categories which are essential for detecting the subtle anomalies, we propose an adaptive noise generator and a multi-scale fusion module for the pre-trained features. MAAE delivers remarkable performances on the benchmark dataset compared with the state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2309.03113.pdf' target='_blank'>https://arxiv.org/pdf/2309.03113.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jubilee Prasad-Rao, Roohollah Heidary, Jesse Williams
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03113">Detecting Manufacturing Defects in PCBs via Data-Centric Machine Learning on Solder Paste Inspection Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automated detection of defects in Printed Circuit Board (PCB) manufacturing using Solder Paste Inspection (SPI) and Automated Optical Inspection (AOI) machines can help improve operational efficiency and significantly reduce the need for manual intervention. In this paper, using SPI-extracted features of 6 million pins, we demonstrate a data-centric approach to train Machine Learning (ML) models to detect PCB defects at three stages of PCB manufacturing. The 6 million PCB pins correspond to 2 million components that belong to 15,387 PCBs. Using a base extreme gradient boosting (XGBoost) ML model, we iterate on the data pre-processing step to improve detection performance. Combining pin-level SPI features using component and PCB IDs, we developed training instances also at the component and PCB level. This allows the ML model to capture any inter-pin, inter-component, or spatial effects that may not be apparent at the pin level. Models are trained at the pin, component, and PCB levels, and the detection results from the different models are combined to identify defective components.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2307.02836.pdf' target='_blank'>https://arxiv.org/pdf/2307.02836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqi Deng, Zhiyu Sun, Ruiyan Zhuang, Jun Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.02836">Noise-to-Norm Reconstruction for Industrial Anomaly Detection and Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Anomaly detection has a wide range of applications and is especially important in industrial quality inspection. Currently, many top-performing anomaly-detection models rely on feature-embedding methods. However, these methods do not perform well on datasets with large variations in object locations. Reconstruction-based methods use reconstruction errors to detect anomalies without considering positional differences between samples. In this study, a reconstruction-based method using the noise-to-norm paradigm is proposed, which avoids the invariant reconstruction of anomalous regions. Our reconstruction network is based on M-net and incorporates multiscale fusion and residual attention modules to enable end-to-end anomaly detection and localization. Experiments demonstrate that the method is effective in reconstructing anomalous regions into normal patterns and achieving accurate anomaly detection and localization. On the MPDD and VisA datasets, our proposed method achieved more competitive results than the latest methods, and it set a new state-of-the-art standard on the MPDD dataset.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2305.13261.pdf' target='_blank'>https://arxiv.org/pdf/2305.13261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Philippe Carvalho, Alexandre Durupt, Yves Grandvalet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13261">A Review of Benchmarks for Visual Defect Detection in the Manufacturing Industry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The field of industrial defect detection using machine learning and deep learning is a subject of active research. Datasets, also called benchmarks, are used to compare and assess research results. There is a number of datasets in industrial visual inspection, of varying quality. Thus, it is a difficult task to determine which dataset to use. Generally speaking, datasets which include a testing set, with precise labeling and made in real-world conditions should be preferred. We propose a study of existing benchmarks to compare and expose their characteristics and their use-cases. A study of industrial metrics requirements, as well as testing procedures, will be presented and applied to the studied benchmarks. We discuss our findings by examining the current state of benchmarks for industrial visual inspection, and by exposing guidelines on the usage of benchmarks.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2304.13963.pdf' target='_blank'>https://arxiv.org/pdf/2304.13963.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Gong, Xiaoqiao Wang, Chichun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13963">Human-machine knowledge hybrid augmentation method for surface defect detection based few-data learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual-based defect detection is a crucial but challenging task in industrial quality control. Most mainstream methods rely on large amounts of existing or related domain data as auxiliary information. However, in actual industrial production, there are often multi-batch, low-volume manufacturing scenarios with rapidly changing task demands, making it difficult to obtain sufficient and diverse defect data. This paper proposes a parallel solution that uses a human-machine knowledge hybrid augmentation method to help the model extract unknown important features. Specifically, by incorporating experts' knowledge of abnormality to create data with rich features, positions, sizes, and backgrounds, we can quickly accumulate an amount of data from scratch and provide it to the model as prior knowledge for few-data learning. The proposed method was evaluated on the magnetic tile dataset and achieved F1-scores of 60.73%, 70.82%, 77.09%, and 82.81% when using 2, 5, 10, and 15 training images, respectively. Compared to the traditional augmentation method's F1-score of 64.59%, the proposed method achieved an 18.22% increase in the best result, demonstrating its feasibility and effectiveness in few-data industrial defect detection.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2303.14535.pdf' target='_blank'>https://arxiv.org/pdf/2303.14535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kilian Batzner, Lars Heckler, Rebecca KÃ¶nig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.14535">EfficientAD: Accurate Visual Anomaly Detection at Millisecond-Level Latencies</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting anomalies in images is an important task, especially in real-time computer vision applications. In this work, we focus on computational efficiency and propose a lightweight feature extractor that processes an image in less than a millisecond on a modern GPU. We then use a student-teacher approach to detect anomalous features. We train a student network to predict the extracted features of normal, i.e., anomaly-free training images. The detection of anomalies at test time is enabled by the student failing to predict their features. We propose a training loss that hinders the student from imitating the teacher feature extractor beyond the normal images. It allows us to drastically reduce the computational cost of the student-teacher model, while improving the detection of anomalous features. We furthermore address the detection of challenging logical anomalies that involve invalid combinations of normal local features, for example, a wrong ordering of objects. We detect these anomalies by efficiently incorporating an autoencoder that analyzes images globally. We evaluate our method, called EfficientAD, on 32 datasets from three industrial anomaly detection dataset collections. EfficientAD sets new standards for both the detection and the localization of anomalies. At a latency of two milliseconds and a throughput of six hundred images per second, it enables a fast handling of anomalies. Together with its low error rate, this makes it an economical solution for real-world applications and a fruitful basis for future research.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2211.12634.pdf' target='_blank'>https://arxiv.org/pdf/2211.12634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaehyeok Bae, Jae-Han Lee, Seyun Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.12634">PNI : Industrial Anomaly Detection using Position and Neighborhood Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Because anomalous samples cannot be used for training, many anomaly detection and localization methods use pre-trained networks and non-parametric modeling to estimate encoded feature distribution. However, these methods neglect the impact of position and neighborhood information on the distribution of normal features. To overcome this, we propose a new algorithm, \textbf{PNI}, which estimates the normal distribution using conditional probability given neighborhood features, modeled with a multi-layer perceptron network. Moreover, position information is utilized by creating a histogram of representative features at each position. Instead of simply resizing the anomaly map, the proposed method employs an additional refine network trained on synthetic anomaly images to better interpolate and account for the shape and edge of the input image. We conducted experiments on the MVTec AD benchmark dataset and achieved state-of-the-art performance, with \textbf{99.56\%} and \textbf{98.98\%} AUROC scores in anomaly detection and localization, respectively.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2207.09792.pdf' target='_blank'>https://arxiv.org/pdf/2207.09792.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianfeng Huang, Chenyang Li, Yimin Lin, Shiguo Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2207.09792">Unsupervised Industrial Anomaly Detection via Pattern Generative and Contrastive Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>It is hard to collect enough flaw images for training deep learning network in industrial production. Therefore, existing industrial anomaly detection methods prefer to use CNN-based unsupervised detection and localization network to achieve this task. However, these methods always fail when there are varieties happened in new signals since traditional end-to-end networks suffer barriers of fitting nonlinear model in high-dimensional space. Moreover, they have a memory library by clustering the feature of normal images essentially, which cause it is not robust to texture change. To this end, we propose the Vision Transformer based (VIT-based) unsupervised anomaly detection network. It utilizes a hierarchical task learning and human experience to enhance its interpretability. Our network consists of pattern generation and comparison networks. Pattern generation network uses two VIT-based encoder modules to extract the feature of two consecutive image patches, then uses VIT-based decoder module to learn the human designed style of these features and predict the third image patch. After this, we use the Siamese-based network to compute the similarity of the generation image patch and original image patch. Finally, we refine the anomaly localization by the bi-directional inference strategy. Comparison experiments on public dataset MVTec dataset show our method achieves 99.8% AUC, which surpasses previous state-of-the-art methods. In addition, we give a qualitative illustration on our own leather and cloth datasets. The accurate segment results strongly prove the accuracy of our method in anomaly detection.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2409.13984.pdf' target='_blank'>https://arxiv.org/pdf/2409.13984.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Geonuk Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13984">Cycle-Consistency Uncertainty Estimation for Visual Prompting based One-Shot Defect Segmentation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industrial defect detection traditionally relies on supervised learning models trained on fixed datasets of known defect types. While effective within a closed set, these models struggle with new, unseen defects, necessitating frequent re-labeling and re-training. Recent advances in visual prompting offer a solution by allowing models to adaptively infer novel categories based on provided visual cues. However, a prevalent issue in these methods is the over-confdence problem, where models can mis-classify unknown objects as known objects with high certainty. To addresssing the fundamental concerns about the adaptability, we propose a solution to estimate uncertainty of the visual prompting process by cycle-consistency. We designed to check whether it can accurately restore the original prompt from its predictions. To quantify this, we measure the mean Intersection over Union (mIoU) between the restored prompt mask and the originally provided prompt mask. Without using complex designs or ensemble methods with multiple networks, our approach achieved a yield rate of 0.9175 in the VISION24 one-shot industrial challenge.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2405.12122.pdf' target='_blank'>https://arxiv.org/pdf/2405.12122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shemonto Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12122">An Active Learning Framework with a Class Balancing Strategy for Time Series Classification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training machine learning models for classification tasks often requires labeling numerous samples, which is costly and time-consuming, especially in time series analysis. This research investigates Active Learning (AL) strategies to reduce the amount of labeled data needed for effective time series classification. Traditional AL techniques cannot control the selection of instances per class for labeling, leading to potential bias in classification performance and instance selection, particularly in imbalanced time series datasets. To address this, we propose a novel class-balancing instance selection algorithm integrated with standard AL strategies. Our approach aims to select more instances from classes with fewer labeled examples, thereby addressing imbalance in time series datasets. We demonstrate the effectiveness of our AL framework in selecting informative data samples for two distinct domains of tactile texture recognition and industrial fault detection. In robotics, our method achieves high-performance texture categorization while significantly reducing labeled training data requirements to 70%. We also evaluate the impact of different sliding window time intervals on robotic texture classification using AL strategies. In synthetic fiber manufacturing, we adapt AL techniques to address the challenge of fault classification, aiming to minimize data annotation cost and time for industries. We also address real-life class imbalances in the multiclass industrial anomalous dataset using our class-balancing instance algorithm integrated with AL strategies. Overall, this thesis highlights the potential of our AL framework across these two distinct domains.
